{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 828,
   "id": "fe949759",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "import datasets\n",
    "import torch\n",
    "import math\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, classification_report\n",
    "from torch.utils.data import TensorDataset\n",
    "import time\n",
    "from itertools import chain\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import torch.nn.functional as F\n",
    "import copy\n",
    "from torch.optim import lr_scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "7a9312a5-7da1-44f7-abad-174bb55393fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import math\n",
    "# this ensures that the current MacOS version is at least 12.3+\n",
    "print(torch.backends.mps.is_available())\n",
    "# this ensures that the current current PyTorch installation was built with MPS activated.\n",
    "print(torch.backends.mps.is_built())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "2e2c2eb4-6228-4f99-98d3-6da34fd4dd54",
   "metadata": {},
   "outputs": [],
   "source": [
    "dtype = torch.float\n",
    "device = torch.device(\"mps\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "id": "759b054e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = datasets.load_dataset(\"conll2003\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2e8eaf8",
   "metadata": {},
   "source": [
    "### Convert words/tokens to indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 683,
   "id": "f4850e59",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "from collections import Counter\n",
    "\n",
    "word_frequency = Counter(itertools.chain(*dataset['train']['tokens']))  # type: ignore\n",
    "\n",
    "# Remove words below threshold 3\n",
    "word_frequency = {\n",
    "    word: frequency\n",
    "    for word, frequency in word_frequency.items()\n",
    "    if frequency >= 3\n",
    "}\n",
    "\n",
    "word2idx = {\n",
    "    word: index\n",
    "    for index, word in enumerate(word_frequency.keys(), start=2)\n",
    "}\n",
    "\n",
    "word2idx['[PAD]'] = 0\n",
    "word2idx['[UNK]'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1004,
   "id": "5b867dbb-a783-4076-ad35-37eb35e959a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['EU', 'rejects', 'German', 'call', 'to', 'boycott', 'British', 'lamb', '.']"
      ]
     },
     "execution_count": 1004,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_tokens = dataset['train'][0]['tokens']\n",
    "sample_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 684,
   "id": "1910d653",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8128"
      ]
     },
     "execution_count": 684,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the vocab size\n",
    "vocab_size = max(word2idx.values())+1\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 689,
   "id": "0ff50b8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c5826c19686471186739c43db27835d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/14041 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b45f3b7732ac4be3a6d538cfd39f97d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3250 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7bd3d2c161d245348beefdabb3d21899",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3453 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def convert_word_to_id(sample):\n",
    "#Code to convert all tokens to their respective indexes\n",
    "#If the token is unknown, we set index of 1\n",
    "    input_ids = [ word2idx.get(token, 1) for token in sample['tokens'] ]\n",
    "\n",
    "    sample['input_ids'] = input_ids\n",
    "    return sample\n",
    "\n",
    "dataset = dataset.map(convert_word_to_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 692,
   "id": "5360f2c0-bf75-4618-9282-a5401a769433",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.DataFrame(dataset['train']).drop(columns=['pos_tags', 'chunk_tags', 'id', 'tokens'])\n",
    "df_train.columns = ['label','input_ids']\n",
    "\n",
    "df_test = pd.DataFrame(dataset['test']).drop(columns=['pos_tags', 'chunk_tags', 'id', 'tokens'])\n",
    "df_test.columns = ['label','input_ids']\n",
    "\n",
    "df_val = pd.DataFrame(dataset['validation']).drop(columns=['pos_tags', 'chunk_tags', 'id', 'tokens'])\n",
    "df_val.columns = ['label','input_ids']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab935b85",
   "metadata": {},
   "source": [
    "### Padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1145,
   "id": "f0e42ff3-55c5-47ee-a9cd-1a385cd7290f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([12, 13]), tensor([5, 0]))\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "# Create a custom Dataset class\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, dataframe):\n",
    "        self.data = dataframe\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        label = torch.tensor(self.data.loc[idx, \"label\"], dtype=torch.long)\n",
    "        input_ids = torch.tensor(self.data.loc[idx, \"input_ids\"], dtype=torch.long)\n",
    "\n",
    "        return input_ids, label\n",
    "\n",
    "# Create an instance of the CustomDataset\n",
    "dataset_train = CustomDataset(df_train)\n",
    "\n",
    "# Example: Accessing a single sample\n",
    "print(dataset_train[2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1146,
   "id": "9733d6c5-271e-4ea8-b045-3375b2b1b5ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_collate(batch):\n",
    "    # Separate input sequences and labels\n",
    "    input_seqs, labels = zip(*batch)\n",
    "    \n",
    "    # Calculate the sequence lengths based on input sequences (assuming they have the same length as labels)\n",
    "    sequence_lengths = [len(seq) for seq in input_seqs]\n",
    "\n",
    "    # Sort input sequences and labels by sequence length (descending)\n",
    "    sorted_seqs_and_labels = sorted(zip(input_seqs, labels), key=lambda x: len(x[0]), reverse=True)\n",
    "    sorted_input_seqs, sorted_labels = zip(*sorted_seqs_and_labels)\n",
    "\n",
    "    # Pad input sequences to the maximum length within the batch\n",
    "    padded_input_seqs = pad_sequence(sorted_input_seqs, batch_first=True, padding_value=0)  # Use 0 as the padding value\n",
    "    padded_labels = pad_sequence(sorted_labels, batch_first=True, padding_value=0)  # Use 0 as the padding value\n",
    "\n",
    "    return padded_input_seqs, padded_labels\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdb31028",
   "metadata": {},
   "source": [
    "### Create dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1147,
   "id": "5cb4f2b9-08ce-4d02-8336-86586fcee0e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataloader_generator(df,shuffle):\n",
    "    dataset_from_df = CustomDataset(df)\n",
    "    batch_size = 64\n",
    "    dataloader = DataLoader(dataset_from_df, batch_size=batch_size, collate_fn=custom_collate, shuffle=shuffle)\n",
    "    return dataloader\n",
    "\n",
    "train_loader  = dataloader_generator(df_train,shuffle=True)\n",
    "test_loader  = dataloader_generator(df_test,shuffle=False)\n",
    "val_loader  = dataloader_generator(df_val,shuffle=False)\n",
    "\n",
    "for batch in val_loader:\n",
    "    input_val, target_val = batch\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ed603a3",
   "metadata": {},
   "source": [
    "### Building the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "id": "05e32ef5-344d-4f13-8023-abf45527db64",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2023-11-06 16:54:57--  https://raw.githubusercontent.com/sighsmile/conlleval/master/conlleval.py\n",
      "正在解析主机 raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
      "正在连接 raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... 已连接。\n",
      "已发出 HTTP 请求，正在等待回应... 200 OK\n",
      "长度：7502 (7.3K) [text/plain]\n",
      "正在保存至: “conlleval.py.1”\n",
      "\n",
      "conlleval.py.1      100%[===================>]   7.33K  --.-KB/s  用时 0s        \n",
      "\n",
      "2023-11-06 16:54:57 (14.7 MB/s) - 已保存 “conlleval.py.1” [7502/7502])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://raw.githubusercontent.com/sighsmile/conlleval/master/conlleval.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "id": "05c1cda1-f710-42ac-a158-784280ed44b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from conlleval import evaluate "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1150,
   "id": "5b253ac4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start training\n",
      "Epoch 1, Loss: 0.24208735396916217, time: 52.45967507362366s\n",
      "validation error: \n",
      "processed 152266 tokens with 5942 phrases; found: 2102 phrases; correct: 1080.\n",
      "accuracy:  20.82%; (non-O)\n",
      "accuracy:  95.34%; precision:  51.38%; recall:  18.18%; FB1:  26.85\n",
      "              LOC: precision:  59.36%; recall:  26.08%; FB1:  36.23  807\n",
      "             MISC: precision:  33.33%; recall:   0.33%; FB1:   0.64  9\n",
      "              ORG: precision:  30.00%; recall:   3.36%; FB1:   6.04  150\n",
      "              PER: precision:  48.68%; recall:  30.02%; FB1:  37.14  1136\n",
      "Epoch 2, Loss: 0.11488994293930856, time: 47.83418798446655s\n",
      "validation error: \n",
      "processed 152266 tokens with 5942 phrases; found: 4661 phrases; correct: 2929.\n",
      "accuracy:  52.38%; (non-O)\n",
      "accuracy:  97.05%; precision:  62.84%; recall:  49.29%; FB1:  55.25\n",
      "              LOC: precision:  72.27%; recall:  65.11%; FB1:  68.50  1655\n",
      "             MISC: precision:  62.36%; recall:  37.20%; FB1:  46.60  550\n",
      "              ORG: precision:  45.43%; recall:  44.44%; FB1:  44.93  1312\n",
      "              PER: precision:  69.41%; recall:  43.11%; FB1:  53.18  1144\n",
      "Epoch 3, Loss: 0.06882393922318111, time: 48.614689111709595s\n",
      "validation error: \n",
      "processed 152266 tokens with 5942 phrases; found: 5102 phrases; correct: 3706.\n",
      "accuracy:  66.22%; (non-O)\n",
      "accuracy:  97.86%; precision:  72.64%; recall:  62.37%; FB1:  67.11\n",
      "              LOC: precision:  85.42%; recall:  69.84%; FB1:  76.85  1502\n",
      "             MISC: precision:  65.60%; recall:  57.70%; FB1:  61.40  811\n",
      "              ORG: precision:  62.67%; recall:  54.21%; FB1:  58.14  1160\n",
      "              PER: precision:  71.45%; recall:  63.19%; FB1:  67.07  1629\n",
      "Epoch 4, Loss: 0.04578833337026564, time: 51.33672094345093s\n",
      "validation error: \n",
      "processed 152266 tokens with 5942 phrases; found: 5280 phrases; correct: 4048.\n",
      "accuracy:  71.87%; (non-O)\n",
      "accuracy:  98.20%; precision:  76.67%; recall:  68.13%; FB1:  72.14\n",
      "              LOC: precision:  89.86%; recall:  74.80%; FB1:  81.64  1529\n",
      "             MISC: precision:  75.29%; recall:  63.45%; FB1:  68.86  777\n",
      "              ORG: precision:  62.88%; recall:  63.16%; FB1:  63.02  1347\n",
      "              PER: precision:  76.34%; recall:  67.43%; FB1:  71.61  1627\n",
      "Epoch 5, Loss: 0.032878815653649245, time: 48.04787993431091s\n",
      "validation error: \n",
      "processed 152266 tokens with 5942 phrases; found: 5393 phrases; correct: 4290.\n",
      "accuracy:  75.31%; (non-O)\n",
      "accuracy:  98.40%; precision:  79.55%; recall:  72.20%; FB1:  75.69\n",
      "              LOC: precision:  85.29%; recall:  80.19%; FB1:  82.66  1727\n",
      "             MISC: precision:  80.28%; recall:  68.87%; FB1:  74.14  791\n",
      "              ORG: precision:  73.72%; recall:  64.21%; FB1:  68.63  1168\n",
      "              PER: precision:  77.39%; recall:  71.72%; FB1:  74.44  1707\n",
      "Epoch 6, Loss: 0.023866635279475964, time: 48.24751901626587s\n",
      "validation error: \n",
      "processed 152266 tokens with 5942 phrases; found: 5465 phrases; correct: 4370.\n",
      "accuracy:  76.53%; (non-O)\n",
      "accuracy:  98.46%; precision:  79.96%; recall:  73.54%; FB1:  76.62\n",
      "              LOC: precision:  88.65%; recall:  79.91%; FB1:  84.05  1656\n",
      "             MISC: precision:  80.98%; recall:  69.74%; FB1:  74.94  794\n",
      "              ORG: precision:  71.36%; recall:  67.26%; FB1:  69.25  1264\n",
      "              PER: precision:  77.50%; recall:  73.67%; FB1:  75.54  1751\n",
      "Epoch 7, Loss: 0.017713668648238208, time: 48.44618272781372s\n",
      "validation error: \n",
      "processed 152266 tokens with 5942 phrases; found: 6117 phrases; correct: 4592.\n",
      "accuracy:  80.59%; (non-O)\n",
      "accuracy:  98.36%; precision:  75.07%; recall:  77.28%; FB1:  76.16\n",
      "              LOC: precision:  85.71%; recall:  82.63%; FB1:  84.15  1771\n",
      "             MISC: precision:  78.41%; recall:  70.50%; FB1:  74.24  829\n",
      "              ORG: precision:  62.20%; recall:  70.92%; FB1:  66.27  1529\n",
      "              PER: precision:  74.09%; recall:  79.97%; FB1:  76.92  1988\n",
      "Epoch 8, Loss: 0.013745928631926125, time: 47.203505992889404s\n",
      "validation error: \n",
      "processed 152266 tokens with 5942 phrases; found: 5289 phrases; correct: 4409.\n",
      "accuracy:  76.44%; (non-O)\n",
      "accuracy:  98.53%; precision:  83.36%; recall:  74.20%; FB1:  78.51\n",
      "              LOC: precision:  91.49%; recall:  80.78%; FB1:  85.81  1622\n",
      "             MISC: precision:  80.94%; recall:  72.78%; FB1:  76.64  829\n",
      "              ORG: precision:  78.06%; recall:  66.07%; FB1:  71.57  1135\n",
      "              PER: precision:  80.33%; recall:  74.27%; FB1:  77.18  1703\n",
      "Epoch 9, Loss: 0.010567720067179338, time: 47.350847005844116s\n",
      "validation error: \n",
      "processed 152266 tokens with 5942 phrases; found: 5375 phrases; correct: 4429.\n",
      "accuracy:  76.74%; (non-O)\n",
      "accuracy:  98.51%; precision:  82.40%; recall:  74.54%; FB1:  78.27\n",
      "              LOC: precision:  91.60%; recall:  81.27%; FB1:  86.13  1630\n",
      "             MISC: precision:  80.24%; recall:  73.54%; FB1:  76.74  845\n",
      "              ORG: precision:  71.33%; recall:  69.20%; FB1:  70.25  1301\n",
      "              PER: precision:  83.18%; recall:  72.20%; FB1:  77.30  1599\n",
      "Epoch 10, Loss: 0.00821540692069737, time: 46.872527837753296s\n",
      "validation error: \n",
      "processed 152266 tokens with 5942 phrases; found: 5612 phrases; correct: 4468.\n",
      "accuracy:  77.35%; (non-O)\n",
      "accuracy:  98.49%; precision:  79.62%; recall:  75.19%; FB1:  77.34\n",
      "              LOC: precision:  84.22%; recall:  84.81%; FB1:  84.51  1850\n",
      "             MISC: precision:  81.68%; recall:  71.58%; FB1:  76.30  808\n",
      "              ORG: precision:  71.56%; recall:  67.56%; FB1:  69.51  1266\n",
      "              PER: precision:  79.62%; recall:  72.96%; FB1:  76.15  1688\n",
      "Epoch 11, Loss: 0.006528246736640788, time: 46.133893966674805s\n",
      "validation error: \n",
      "processed 152266 tokens with 5942 phrases; found: 6042 phrases; correct: 4622.\n",
      "accuracy:  80.33%; (non-O)\n",
      "accuracy:  98.43%; precision:  76.50%; recall:  77.79%; FB1:  77.14\n",
      "              LOC: precision:  86.12%; recall:  83.78%; FB1:  84.93  1787\n",
      "             MISC: precision:  71.69%; recall:  75.27%; FB1:  73.44  968\n",
      "              ORG: precision:  71.24%; recall:  68.53%; FB1:  69.86  1290\n",
      "              PER: precision:  73.61%; recall:  79.80%; FB1:  76.58  1997\n",
      "Epoch 12, Loss: 0.00525556694959629, time: 48.15459370613098s\n",
      "validation error: \n",
      "processed 152266 tokens with 5942 phrases; found: 5796 phrases; correct: 4560.\n",
      "accuracy:  79.07%; (non-O)\n",
      "accuracy:  98.49%; precision:  78.67%; recall:  76.74%; FB1:  77.70\n",
      "              LOC: precision:  86.47%; recall:  83.12%; FB1:  84.76  1766\n",
      "             MISC: precision:  79.93%; recall:  73.43%; FB1:  76.54  847\n",
      "              ORG: precision:  68.11%; recall:  70.40%; FB1:  69.23  1386\n",
      "              PER: precision:  78.58%; recall:  76.66%; FB1:  77.60  1797\n",
      "Epoch 13, Loss: 0.00421205094052394, time: 47.617199182510376s\n",
      "validation error: \n",
      "processed 152266 tokens with 5942 phrases; found: 5846 phrases; correct: 4574.\n",
      "accuracy:  79.32%; (non-O)\n",
      "accuracy:  98.51%; precision:  78.24%; recall:  76.98%; FB1:  77.60\n",
      "              LOC: precision:  86.80%; recall:  83.02%; FB1:  84.86  1757\n",
      "             MISC: precision:  80.05%; recall:  73.54%; FB1:  76.65  847\n",
      "              ORG: precision:  67.91%; recall:  70.40%; FB1:  69.13  1390\n",
      "              PER: precision:  77.05%; recall:  77.47%; FB1:  77.26  1852\n",
      "Epoch 14, Loss: 0.0038494242876450616, time: 47.95925307273865s\n",
      "validation error: \n",
      "processed 152266 tokens with 5942 phrases; found: 5703 phrases; correct: 4530.\n",
      "accuracy:  78.86%; (non-O)\n",
      "accuracy:  98.53%; precision:  79.43%; recall:  76.24%; FB1:  77.80\n",
      "              LOC: precision:  87.94%; recall:  82.96%; FB1:  85.38  1733\n",
      "             MISC: precision:  77.65%; recall:  75.38%; FB1:  76.50  895\n",
      "              ORG: precision:  73.76%; recall:  67.71%; FB1:  70.61  1231\n",
      "              PER: precision:  76.08%; recall:  76.17%; FB1:  76.13  1844\n",
      "Epoch 15, Loss: 0.0037216038500032895, time: 49.66430187225342s\n",
      "validation error: \n",
      "processed 152266 tokens with 5942 phrases; found: 5539 phrases; correct: 4432.\n",
      "accuracy:  77.23%; (non-O)\n",
      "accuracy:  98.50%; precision:  80.01%; recall:  74.59%; FB1:  77.21\n",
      "              LOC: precision:  88.67%; recall:  82.25%; FB1:  85.34  1704\n",
      "             MISC: precision:  73.08%; recall:  74.19%; FB1:  73.63  936\n",
      "              ORG: precision:  73.55%; recall:  67.19%; FB1:  70.23  1225\n",
      "              PER: precision:  79.81%; recall:  72.53%; FB1:  76.00  1674\n",
      "Epoch 16, Loss: 0.003172349494839595, time: 49.49557089805603s\n",
      "validation error: \n",
      "processed 152266 tokens with 5942 phrases; found: 5817 phrases; correct: 4554.\n",
      "accuracy:  78.83%; (non-O)\n",
      "accuracy:  98.49%; precision:  78.29%; recall:  76.64%; FB1:  77.46\n",
      "              LOC: precision:  85.91%; recall:  83.61%; FB1:  84.74  1788\n",
      "             MISC: precision:  74.62%; recall:  74.30%; FB1:  74.46  918\n",
      "              ORG: precision:  71.13%; recall:  68.53%; FB1:  69.81  1292\n",
      "              PER: precision:  77.74%; recall:  76.76%; FB1:  77.25  1819\n",
      "Epoch 17, Loss: 0.002620459050971972, time: 47.30130100250244s\n",
      "validation error: \n",
      "processed 152266 tokens with 5942 phrases; found: 5804 phrases; correct: 4577.\n",
      "accuracy:  79.24%; (non-O)\n",
      "accuracy:  98.50%; precision:  78.86%; recall:  77.03%; FB1:  77.93\n",
      "              LOC: precision:  87.66%; recall:  83.51%; FB1:  85.53  1750\n",
      "             MISC: precision:  76.78%; recall:  73.86%; FB1:  75.29  887\n",
      "              ORG: precision:  69.38%; recall:  70.47%; FB1:  69.92  1362\n",
      "              PER: precision:  78.50%; recall:  76.93%; FB1:  77.71  1805\n",
      "Epoch 18, Loss: 0.00238024852177742, time: 47.947713136672974s\n",
      "validation error: \n",
      "processed 152266 tokens with 5942 phrases; found: 5805 phrases; correct: 4541.\n",
      "accuracy:  78.55%; (non-O)\n",
      "accuracy:  98.47%; precision:  78.23%; recall:  76.42%; FB1:  77.31\n",
      "              LOC: precision:  85.42%; recall:  84.21%; FB1:  84.81  1811\n",
      "             MISC: precision:  76.91%; recall:  72.99%; FB1:  74.90  875\n",
      "              ORG: precision:  69.67%; recall:  70.25%; FB1:  69.96  1352\n",
      "              PER: precision:  78.04%; recall:  74.86%; FB1:  76.42  1767\n",
      "Epoch 19, Loss: 0.002281301094626542, time: 48.1134819984436s\n",
      "validation error: \n",
      "processed 152266 tokens with 5942 phrases; found: 5658 phrases; correct: 4538.\n",
      "accuracy:  78.69%; (non-O)\n",
      "accuracy:  98.56%; precision:  80.21%; recall:  76.37%; FB1:  78.24\n",
      "              LOC: precision:  87.74%; recall:  84.16%; FB1:  85.91  1762\n",
      "             MISC: precision:  77.63%; recall:  73.75%; FB1:  75.64  876\n",
      "              ORG: precision:  75.79%; recall:  67.93%; FB1:  71.65  1202\n",
      "              PER: precision:  77.06%; recall:  76.06%; FB1:  76.56  1818\n",
      "Epoch 20, Loss: 0.002566172640349991, time: 48.22484111785889s\n",
      "validation error: \n",
      "processed 152266 tokens with 5942 phrases; found: 5651 phrases; correct: 4545.\n",
      "accuracy:  78.65%; (non-O)\n",
      "accuracy:  98.55%; precision:  80.43%; recall:  76.49%; FB1:  78.41\n",
      "              LOC: precision:  88.78%; recall:  83.56%; FB1:  86.09  1729\n",
      "             MISC: precision:  78.97%; recall:  74.95%; FB1:  76.91  875\n",
      "              ORG: precision:  74.96%; recall:  67.64%; FB1:  71.11  1210\n",
      "              PER: precision:  76.86%; recall:  76.66%; FB1:  76.76  1837\n"
     ]
    }
   ],
   "source": [
    "class BiLSTMNER(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, num_layers, dropout):\n",
    "        super(BiLSTMNER, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.bilstm = nn.LSTM(embedding_dim, hidden_dim, num_layers=num_layers, \n",
    "                              batch_first=True, bidirectional=True)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.linear = nn.Linear(hidden_dim * 2, output_dim)\n",
    "        self.elu = nn.ELU()\n",
    "        self.classifier = nn.Linear(output_dim, num_tags)  # num_tags is the number of unique NER tags\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x, _ = self.bilstm(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.linear(x)\n",
    "        x = self.elu(x)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "#initialize \n",
    "num_tags = 9\n",
    "vocab_size = max(word2idx.values())+1\n",
    "\n",
    "model = BiLSTMNER(vocab_size, 100, 256, 128, 1, 0.33) \n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "\n",
    "#training\n",
    "num_epochs = 20\n",
    "print('start training')\n",
    "for epoch in range(num_epochs):\n",
    "    start_time = time.time()\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        inputs, targets = batch\n",
    "        outputs = model(inputs)\n",
    "        batch_size = inputs.size()[-1]    \n",
    "        #From the instruction of CrossEntropy, we need to change the format of outputs \n",
    "        loss = loss_function(outputs.permute(0,2,1), targets) \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    end_time = time.time()\n",
    "    print(f'Epoch {epoch + 1}, Loss: {total_loss / len(train_loader)}, time: {end_time-start_time}s')\n",
    "    print('validation error: ')\n",
    "    precision, recall, f1 = eval(model, val_loader)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1162,
   "id": "33e2dc72-5c1c-4776-8483-a6d5d5ee0d64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision = 80.42824278888693, recall = 76.48939750925614, f1 = 78.40938497369102\n"
     ]
    }
   ],
   "source": [
    "print(f\"Validation: precision = {precision}, recall = {recall}, f1 = {f1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1151,
   "id": "2472920f-eef3-4e13-b7b9-c5218650840b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SAVE THE MODEL\n",
    "torch.save(model.state_dict(), 'task1.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 444,
   "id": "3d45cc5b-8183-41e7-95d2-f595cd24edea",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Example reversed_ner_tags dictionary\n",
    "reversed_ner_tags = {\n",
    "    0: 'O',\n",
    "    1: 'B-PER',\n",
    "    2: 'I-PER',\n",
    "    3: 'B-ORG',\n",
    "    4: 'I-ORG',\n",
    "    5: 'B-LOC',\n",
    "    6: 'I-LOC',\n",
    "    7: 'B-MISC',\n",
    "    8: 'I-MISC'\n",
    "}\n",
    "\n",
    "# Example tensor with shape (32, 36)\n",
    "tensor = torch.randint(0, 9, (32, 36))  # Random integers between 0 and 8\n",
    "\n",
    "# Map tensor elements using reversed_ner_tags\n",
    "mapped_tensor = [[reversed_ner_tags[item.item()] for item in row] for row in tensor]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "id": "06214e1e-da38-4c52-92aa-eeffcc6c7ded",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'O',\n",
       " 1: 'B-PER',\n",
       " 2: 'I-PER',\n",
       " 3: 'B-ORG',\n",
       " 4: 'I-ORG',\n",
       " 5: 'B-LOC',\n",
       " 6: 'I-LOC',\n",
       " 7: 'B-MISC',\n",
       " 8: 'I-MISC'}"
      ]
     },
     "execution_count": 398,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ner_tags = {'O': 0, 'B-PER': 1, 'I-PER': 2, 'B-ORG': 3, 'I-ORG': 4, 'B-LOC': 5, 'I-LOC': 6, 'B-MISC': 7, 'I-MISC': 8}\n",
    "\n",
    "reversed_ner_tags = {value: key for key, value in ner_tags.items()}\n",
    "reversed_ner_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1149,
   "id": "7c48799c-6972-4695-b643-e350cf5449bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#evaluation\n",
    "def eval(model, loader):\n",
    "    model.eval()\n",
    "    all_preds, all_labels = [], []\n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            inputs, targets = batch\n",
    "            outputs = model(inputs)\n",
    "            _, preds = torch.max(outputs, -1)\n",
    "            preds_converted = [[reversed_ner_tags[item.item()] for item in row] for row in preds]\n",
    "            targets_converted = [[reversed_ner_tags[item.item()] for item in row] for row in targets]\n",
    "            all_preds.extend(preds_converted)\n",
    "            all_labels.extend(targets_converted)\n",
    "    # all_preds = list(chain.from_iterable(all_preds))\n",
    "    # all_labels = list(chain.from_iterable(all_labels))\n",
    "    # all_labels = torch.cat(all_labels)\n",
    "    all_preds = itertools.chain(*all_preds)    \n",
    "    all_labels =itertools.chain(*all_labels)\n",
    "    result = evaluate(all_labels, all_preds,verbose=True)\n",
    "    precision, recall, f1 = result[0], result[1],result[2]\n",
    "    return precision, recall, f1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1163,
   "id": "f9019f36-fb4e-4c88-8068-22fb5571b43d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test: \n",
      "processed 146937 tokens with 5648 phrases; found: 5146 phrases; correct: 3710.\n",
      "accuracy:  70.02%; (non-O)\n",
      "accuracy:  97.95%; precision:  72.09%; recall:  65.69%; FB1:  68.74\n",
      "              LOC: precision:  84.52%; recall:  75.30%; FB1:  79.64  1486\n",
      "             MISC: precision:  64.47%; recall:  62.82%; FB1:  63.64  684\n",
      "              ORG: precision:  67.13%; recall:  57.80%; FB1:  62.12  1430\n",
      "              PER: precision:  68.11%; recall:  65.12%; FB1:  66.58  1546\n"
     ]
    }
   ],
   "source": [
    "print('Test: ')\n",
    "precision, recall, f1 = eval(model, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1164,
   "id": "78abd59a-f168-49c7-9ff7-83b792f2804d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test: precision = 72.09483093664983, recall = 65.68696883852692, f1 = 68.74189364461739\n"
     ]
    }
   ],
   "source": [
    "print(f\"Test: precision = {precision}, recall = {recall}, f1 = {f1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9c7b196-9fe2-4c2a-b4ec-d9dfc087305d",
   "metadata": {},
   "source": [
    "### Solution for the task 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee49a0af-53dd-4ba8-ba90-aec447208984",
   "metadata": {},
   "source": [
    "1. Hyperparameters:\n",
    "- vocab_size = 8128\n",
    "- embedding_dim = 100\n",
    "- hidden_dim = 256 \n",
    "- output_dim = 128\n",
    "- num_layers = 1\n",
    "- dropout = 0.33\n",
    "- optimizer learning rate= 0.001\n",
    "- batch_size = 64\n",
    "2. Solution:\n",
    "  At first, I created a vocab that maps all the tokens from the training set to a number, and I gave up the tokens that appeared less than 3 times. Secondly, I custimized a dataset class so that each batch will conatin (input_ids, ner_tags). Next, I used padding_sequence to customize the padding value of 0 in input and 9 in ner_tags. Why do I pad here? I need to make sure for each batch, which contains 32 samples, will have the max_length within one batch. Thirdly, I designed my bilstm model. The model will firstly embed all the inputs to 100-dim vectors and then throw the vectors to the lstm layer. Through elu, dropout, and one more linear layer, it model will predict the name entity for each token in samples.\n",
    "3. Questions and answers:\n",
    "- What are the precision, recall, and F1 score on the validation data?\n",
    "- precision = 80.42824278888693, recall = 76.48939750925614, f1 = 78.40938497369102\r",
    "- What are the precision, recall, and F1 score on the test data?\n",
    "- precision = 72.09483093664983, recall = 65.68696883852692, f1 = 68.74189364461739\r"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ba6fdc8-8597-44e6-ab80-385f377a2d8d",
   "metadata": {},
   "source": [
    "## Task 2: Glove Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0c58016-684c-42be-9fbb-4a425290b1e6",
   "metadata": {},
   "source": [
    "### Load Glove Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0a4787e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to load GloVe embeddings from a file\n",
    "def load_glove_embeddings(file_path):\n",
    "    embeddings_index = {}\n",
    "    with open(file_path, encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            coefs = np.asarray(values[1:], dtype=\"float32\")\n",
    "            embeddings_index[word] = coefs\n",
    "    return embeddings_index\n",
    "\n",
    "# Specify the path to your downloaded \"glove.6B.100d.txt\" file\n",
    "glove_file_path = \"glove.6B.100d\"\n",
    "\n",
    "# Load GloVe embeddings into memory\n",
    "glove_embeddings = load_glove_embeddings(glove_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c2ab1b3-055a-4e9b-8253-8240ca6c02f0",
   "metadata": {},
   "source": [
    "### Create Glove Idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1027,
   "id": "c1629568-d8dc-4a6b-a09a-40de7421d676",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df6736926de741e0b33edb55c7bcac30",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/14041 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30d637d0c06f4c37baad8bb198e377e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3250 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b051f829e2c54665b8f8f20ca180afd1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3453 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def convert_word_to_glove_ids(sample):\n",
    "    tokens = sample['tokens']\n",
    "    glove_ids =[]\n",
    "    for token in tokens:\n",
    "        token = token.lower()\n",
    "        indices = np.where(vocab_npa == token)\n",
    "        if indices[0].size > 0:\n",
    "            index = indices[0][0]\n",
    "        else:\n",
    "            index = 1\n",
    "        glove_ids.append(index)\n",
    "    sample['glove_ids'] = glove_ids\n",
    "    return sample\n",
    "dataset = dataset.map(convert_word_to_glove_ids)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edb202b9-0835-4d9d-baf6-ec97a9e6ad06",
   "metadata": {},
   "source": [
    "### Customize the layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 831,
   "id": "934b5706-5dec-498d-9f3f-59366de8f3eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert glove into a layer\n",
    "vocab,embeddings = [],[]\n",
    "with open('glove.6B.100d',encoding=\"utf-8\") as fi:\n",
    "    full_content = fi.read().strip().split('\\n')\n",
    "for i in range(len(full_content)):\n",
    "    i_word = full_content[i].split(' ')[0]\n",
    "    i_embeddings = [float(val) for val in full_content[i].split(' ')[1:]]\n",
    "    vocab.append(i_word)\n",
    "    embeddings.append(i_embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 832,
   "id": "ae3b857c-1bda-40a4-9d00-c2cc8a83847c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<pad>' '<unk>' 'the' ',' '.' 'of' 'to' 'and' 'in' 'a']\n",
      "(400002, 100)\n"
     ]
    }
   ],
   "source": [
    "vocab_npa = np.array(vocab)\n",
    "embs_npa = np.array(embeddings)\n",
    "\n",
    "#insert '<pad>' and '<unk>' tokens at start of vocab_npa.\n",
    "vocab_npa = np.insert(vocab_npa, 0, '<pad>')\n",
    "vocab_npa = np.insert(vocab_npa, 1, '<unk>')\n",
    "print(vocab_npa[:10])\n",
    "\n",
    "pad_emb_npa = np.zeros((1,embs_npa.shape[1]))   #embedding for '<pad>' token.\n",
    "unk_emb_npa = np.mean(embs_npa,axis=0,keepdims=True)    #embedding for '<unk>' token.\n",
    "\n",
    "#insert embeddings for pad and unk tokens at top of embs_npa.\n",
    "embs_npa = np.vstack((pad_emb_npa,unk_emb_npa,embs_npa))\n",
    "print(embs_npa.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 877,
   "id": "22fbbef0-211c-408a-b8af-e8ab64d282dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([400002, 100])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "my_embedding_layer = torch.nn.Embedding.from_pretrained(torch.from_numpy(embs_npa).float(),freeze=True)\n",
    "\n",
    "assert my_embedding_layer.weight.shape == embs_npa.shape\n",
    "print(my_embedding_layer.weight.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1c9e037-fb69-4a54-bd30-974c5d05d41b",
   "metadata": {},
   "source": [
    "### Make Glove case-sensitive -- creating another feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 858,
   "id": "fca5158d-21c9-4eee-9500-d8da43d28fa6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4886c4d5aff9468793ddde6b0f1bc634",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/14041 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "afd6a197c12b4f349a8b8890a51df08a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3250 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5fdd47f2bcc457b96b30c8e68d82d77",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3453 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#add features to the dataloader \n",
    "#case 0: lower case - no uppercase\n",
    "#case 1: first word is uppercase\n",
    "#case 2: whole word is uppeercase\n",
    "#case 3: others: e.g. \",\"\n",
    "def capital_case(word):\n",
    "    if word.islower():\n",
    "        return 0\n",
    "    elif word.isupper():\n",
    "        return 2\n",
    "    elif word.istitle():\n",
    "        return 1\n",
    "    else: return 3\n",
    "\n",
    "def convert_word_to_capital_case(sample):\n",
    "    capitals = [capital_case(word) for word in sample['tokens'] ]\n",
    "    sample['capitals'] =capitals\n",
    "    return sample \n",
    "\n",
    "dataset = dataset.map(convert_word_to_capital_case)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1023,
   "id": "8e2eb2d5-dbff-4330-b6f3-4656fdabb82c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '2',\n",
       " 'tokens': ['BRUSSELS', '1996-08-22'],\n",
       " 'pos_tags': [22, 11],\n",
       " 'chunk_tags': [11, 12],\n",
       " 'ner_tags': [5, 0],\n",
       " 'input_ids': [12, 13],\n",
       " 'capitals': [2, 3],\n",
       " 'glove_ids': [1, 1]}"
      ]
     },
     "execution_count": 1023,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train'][2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de631f84-db7f-4c05-adac-d14e17453b36",
   "metadata": {},
   "source": [
    "### Padding -- glove embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1165,
   "id": "e4517d97-f5a7-4e7c-bbf1-248a90ee29b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([3, 0, 7, 0, 0, 0, 7, 0, 0]), tensor([  646,  7580,   516,   582,     6,  5262,   299, 10240,     4]), tensor([2, 0, 1, 0, 0, 0, 1, 0, 3]))\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "# Create a custom Dataset class\n",
    "class CustomDataset(Dataset):\n",
    "    \n",
    "    def __init__(self,data):\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        label = torch.tensor(self.data[index]['ner_tags'], dtype=torch.long ) \n",
    "        glove_ids = torch.tensor(self.data[index]['glove_ids'], dtype=torch.long)\n",
    "        capital = torch.tensor(self.data[index]['capitals'], dtype=torch.long)\n",
    "        \n",
    "        return label, glove_ids, capital\n",
    "\n",
    "# Create an instance of the CustomDataset\n",
    "dataset_train = CustomDataset(dataset['train'])\n",
    "dataset_test = CustomDataset(dataset['test'])\n",
    "dataset_val = CustomDataset(dataset['validation'])\n",
    "\n",
    "# Example: Accessing a single sample\n",
    "print(dataset_train[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1166,
   "id": "7ca68deb-5d45-411a-8d74-adca3b7b1a76",
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_collate(batch):\n",
    "    label, glove_ids, capital = zip(*batch)\n",
    "    padded_label = pad_sequence(label, batch_first=True, padding_value=9 )\n",
    "    padded_glove_ids = pad_sequence(glove_ids, batch_first=True, padding_value=0 )\n",
    "    padded_capital = pad_sequence(capital, batch_first=True, padding_value=4 )\n",
    "    return padded_glove_ids, padded_capital, padded_label\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1167,
   "id": "6a53c811-7114-4fbf-87de-810107a7eaa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "train_loader = DataLoader(dataset_train, batch_size=batch_size, collate_fn= custom_collate, shuffle=True)\n",
    "test_loader = DataLoader(dataset_test, batch_size=batch_size, collate_fn= custom_collate, shuffle=False)\n",
    "val_loader = DataLoader(dataset_val, batch_size=batch_size, collate_fn= custom_collate, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1174,
   "id": "32a2a985-9678-4920-a873-1ce100897623",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start training\n",
      "Epoch 1, Loss: 0.2787948575378819, time: 59.194642066955566s\n",
      "validation error: \n",
      "processed 51362 tokens with 5942 phrases; found: 6183 phrases; correct: 5010.\n",
      "accuracy:  85.81%; (non-O)\n",
      "accuracy:  97.16%; precision:  81.03%; recall:  84.32%; FB1:  82.64\n",
      "              LOC: precision:  83.66%; recall:  90.85%; FB1:  87.11  1995\n",
      "             MISC: precision:  68.96%; recall:  74.95%; FB1:  71.83  1002\n",
      "              ORG: precision:  72.70%; recall:  70.69%; FB1:  71.68  1304\n",
      "              PER: precision:  90.44%; recall:  92.40%; FB1:  91.41  1882\n",
      "Epoch 2, Loss: 0.0857482789422978, time: 59.39733099937439s\n",
      "validation error: \n",
      "processed 51362 tokens with 5942 phrases; found: 6033 phrases; correct: 5285.\n",
      "accuracy:  89.63%; (non-O)\n",
      "accuracy:  97.99%; precision:  87.60%; recall:  88.94%; FB1:  88.27\n",
      "              LOC: precision:  92.77%; recall:  92.16%; FB1:  92.46  1825\n",
      "             MISC: precision:  79.46%; recall:  79.28%; FB1:  79.37  920\n",
      "              ORG: precision:  78.05%; recall:  84.04%; FB1:  80.93  1444\n",
      "              PER: precision:  94.03%; recall:  94.14%; FB1:  94.09  1844\n",
      "Epoch 3, Loss: 0.06427998816255819, time: 58.05035185813904s\n",
      "validation error: \n",
      "processed 51362 tokens with 5942 phrases; found: 6028 phrases; correct: 5366.\n",
      "accuracy:  91.06%; (non-O)\n",
      "accuracy:  98.25%; precision:  89.02%; recall:  90.31%; FB1:  89.66\n",
      "              LOC: precision:  94.65%; recall:  92.49%; FB1:  93.56  1795\n",
      "             MISC: precision:  80.36%; recall:  82.54%; FB1:  81.43  947\n",
      "              ORG: precision:  81.43%; recall:  85.98%; FB1:  83.64  1416\n",
      "              PER: precision:  93.74%; recall:  95.17%; FB1:  94.45  1870\n",
      "Epoch 4, Loss: 0.05143080727959221, time: 58.354299783706665s\n",
      "validation error: \n",
      "processed 51362 tokens with 5942 phrases; found: 6084 phrases; correct: 5425.\n",
      "accuracy:  92.00%; (non-O)\n",
      "accuracy:  98.33%; precision:  89.17%; recall:  91.30%; FB1:  90.22\n",
      "              LOC: precision:  93.02%; recall:  95.75%; FB1:  94.37  1891\n",
      "             MISC: precision:  78.62%; recall:  83.73%; FB1:  81.09  982\n",
      "              ORG: precision:  84.84%; recall:  83.89%; FB1:  84.36  1326\n",
      "              PER: precision:  93.85%; recall:  96.04%; FB1:  94.93  1885\n",
      "Epoch 5, Loss: 0.04187326981178061, time: 59.04758620262146s\n",
      "validation error: \n",
      "processed 51362 tokens with 5942 phrases; found: 6045 phrases; correct: 5450.\n",
      "accuracy:  92.28%; (non-O)\n",
      "accuracy:  98.47%; precision:  90.16%; recall:  91.72%; FB1:  90.93\n",
      "              LOC: precision:  93.03%; recall:  95.16%; FB1:  94.08  1879\n",
      "             MISC: precision:  83.41%; recall:  82.86%; FB1:  83.13  916\n",
      "              ORG: precision:  84.62%; recall:  87.40%; FB1:  85.99  1385\n",
      "              PER: precision:  94.69%; recall:  95.87%; FB1:  95.28  1865\n",
      "Epoch 6, Loss: 0.03367272468114441, time: 58.262818813323975s\n",
      "validation error: \n",
      "processed 51362 tokens with 5942 phrases; found: 6143 phrases; correct: 5483.\n",
      "accuracy:  93.15%; (non-O)\n",
      "accuracy:  98.44%; precision:  89.26%; recall:  92.28%; FB1:  90.74\n",
      "              LOC: precision:  94.36%; recall:  95.59%; FB1:  94.97  1861\n",
      "             MISC: precision:  79.81%; recall:  82.75%; FB1:  81.26  956\n",
      "              ORG: precision:  82.64%; recall:  89.49%; FB1:  85.93  1452\n",
      "              PER: precision:  94.13%; recall:  95.77%; FB1:  94.94  1874\n",
      "Epoch 7, Loss: 0.027345544093457814, time: 59.46216917037964s\n",
      "validation error: \n",
      "processed 51362 tokens with 5942 phrases; found: 6015 phrases; correct: 5484.\n",
      "accuracy:  92.49%; (non-O)\n",
      "accuracy:  98.54%; precision:  91.17%; recall:  92.29%; FB1:  91.73\n",
      "              LOC: precision:  94.70%; recall:  95.32%; FB1:  95.01  1849\n",
      "             MISC: precision:  82.86%; recall:  86.01%; FB1:  84.41  957\n",
      "              ORG: precision:  87.71%; recall:  87.84%; FB1:  87.78  1343\n",
      "              PER: precision:  94.43%; recall:  95.66%; FB1:  95.04  1866\n",
      "Epoch 8, Loss: 0.021476747551721267, time: 58.77496004104614s\n",
      "validation error: \n",
      "processed 51362 tokens with 5942 phrases; found: 6044 phrases; correct: 5502.\n",
      "accuracy:  92.69%; (non-O)\n",
      "accuracy:  98.55%; precision:  91.03%; recall:  92.60%; FB1:  91.81\n",
      "              LOC: precision:  94.57%; recall:  95.75%; FB1:  95.16  1860\n",
      "             MISC: precision:  85.90%; recall:  83.95%; FB1:  84.91  901\n",
      "              ORG: precision:  84.22%; recall:  89.93%; FB1:  86.98  1432\n",
      "              PER: precision:  95.25%; recall:  95.71%; FB1:  95.48  1851\n",
      "Epoch 9, Loss: 0.01826574724400416, time: 60.954275131225586s\n",
      "validation error: \n",
      "processed 51362 tokens with 5942 phrases; found: 6081 phrases; correct: 5504.\n",
      "accuracy:  93.13%; (non-O)\n",
      "accuracy:  98.58%; precision:  90.51%; recall:  92.63%; FB1:  91.56\n",
      "              LOC: precision:  95.46%; recall:  94.99%; FB1:  95.23  1828\n",
      "             MISC: precision:  82.56%; recall:  84.71%; FB1:  83.62  946\n",
      "              ORG: precision:  84.06%; recall:  90.83%; FB1:  87.31  1449\n",
      "              PER: precision:  94.73%; recall:  95.55%; FB1:  95.14  1858\n",
      "Epoch 10, Loss: 0.013936886461239986, time: 64.49255204200745s\n",
      "validation error: \n",
      "processed 51362 tokens with 5942 phrases; found: 6052 phrases; correct: 5498.\n",
      "accuracy:  92.89%; (non-O)\n",
      "accuracy:  98.58%; precision:  90.85%; recall:  92.53%; FB1:  91.68\n",
      "              LOC: precision:  95.55%; recall:  94.67%; FB1:  95.11  1820\n",
      "             MISC: precision:  81.27%; recall:  87.53%; FB1:  84.28  993\n",
      "              ORG: precision:  86.99%; recall:  87.77%; FB1:  87.38  1353\n",
      "              PER: precision:  94.11%; recall:  96.36%; FB1:  95.23  1886\n",
      "Epoch 11, Loss: 0.010753243909725412, time: 63.68161940574646s\n",
      "validation error: \n",
      "processed 51362 tokens with 5942 phrases; found: 6062 phrases; correct: 5484.\n",
      "accuracy:  92.71%; (non-O)\n",
      "accuracy:  98.53%; precision:  90.47%; recall:  92.29%; FB1:  91.37\n",
      "              LOC: precision:  94.12%; recall:  95.92%; FB1:  95.01  1872\n",
      "             MISC: precision:  83.39%; recall:  86.01%; FB1:  84.68  951\n",
      "              ORG: precision:  84.46%; recall:  87.55%; FB1:  85.98  1390\n",
      "              PER: precision:  94.92%; recall:  95.28%; FB1:  95.10  1849\n",
      "Epoch 12, Loss: 0.009369476515249433, time: 61.54601192474365s\n",
      "validation error: \n",
      "processed 51362 tokens with 5942 phrases; found: 6029 phrases; correct: 5496.\n",
      "accuracy:  92.99%; (non-O)\n",
      "accuracy:  98.62%; precision:  91.16%; recall:  92.49%; FB1:  91.82\n",
      "              LOC: precision:  95.07%; recall:  95.43%; FB1:  95.25  1844\n",
      "             MISC: precision:  81.38%; recall:  86.77%; FB1:  83.99  983\n",
      "              ORG: precision:  87.74%; recall:  87.02%; FB1:  87.38  1330\n",
      "              PER: precision:  94.87%; recall:  96.42%; FB1:  95.64  1872\n",
      "Epoch 13, Loss: 0.007336435311431573, time: 62.67510199546814s\n",
      "validation error: \n",
      "processed 51362 tokens with 5942 phrases; found: 6031 phrases; correct: 5527.\n",
      "accuracy:  93.33%; (non-O)\n",
      "accuracy:  98.66%; precision:  91.64%; recall:  93.02%; FB1:  92.32\n",
      "              LOC: precision:  94.47%; recall:  96.79%; FB1:  95.62  1882\n",
      "             MISC: precision:  83.98%; recall:  84.71%; FB1:  84.34  930\n",
      "              ORG: precision:  87.96%; recall:  89.86%; FB1:  88.90  1370\n",
      "              PER: precision:  95.35%; recall:  95.71%; FB1:  95.53  1849\n",
      "Epoch 14, Loss: 0.006210239743284712, time: 63.491557121276855s\n",
      "validation error: \n",
      "processed 51362 tokens with 5942 phrases; found: 6091 phrases; correct: 5548.\n",
      "accuracy:  93.79%; (non-O)\n",
      "accuracy:  98.69%; precision:  91.09%; recall:  93.37%; FB1:  92.21\n",
      "              LOC: precision:  95.59%; recall:  95.65%; FB1:  95.62  1838\n",
      "             MISC: precision:  85.22%; recall:  86.33%; FB1:  85.78  934\n",
      "              ORG: precision:  85.04%; recall:  91.13%; FB1:  87.98  1437\n",
      "              PER: precision:  94.21%; recall:  96.25%; FB1:  95.22  1882\n",
      "Epoch 15, Loss: 0.0048910202573593286, time: 89.17146420478821s\n",
      "validation error: \n",
      "processed 51362 tokens with 5942 phrases; found: 6044 phrases; correct: 5511.\n",
      "accuracy:  93.11%; (non-O)\n",
      "accuracy:  98.61%; precision:  91.18%; recall:  92.75%; FB1:  91.96\n",
      "              LOC: precision:  94.52%; recall:  95.86%; FB1:  95.19  1863\n",
      "             MISC: precision:  86.20%; recall:  86.01%; FB1:  86.10  920\n",
      "              ORG: precision:  87.30%; recall:  87.62%; FB1:  87.46  1346\n",
      "              PER: precision:  93.05%; recall:  96.74%; FB1:  94.86  1915\n",
      "Epoch 16, Loss: 0.00643773535636931, time: 87.57236385345459s\n",
      "validation error: \n",
      "processed 51362 tokens with 5942 phrases; found: 6053 phrases; correct: 5531.\n",
      "accuracy:  93.32%; (non-O)\n",
      "accuracy:  98.64%; precision:  91.38%; recall:  93.08%; FB1:  92.22\n",
      "              LOC: precision:  94.51%; recall:  96.46%; FB1:  95.47  1875\n",
      "             MISC: precision:  84.08%; recall:  85.36%; FB1:  84.71  936\n",
      "              ORG: precision:  88.36%; recall:  88.29%; FB1:  88.33  1340\n",
      "              PER: precision:  94.01%; recall:  97.07%; FB1:  95.51  1902\n",
      "Epoch 17, Loss: 0.004882407614059048, time: 63.116442918777466s\n",
      "validation error: \n",
      "processed 51362 tokens with 5942 phrases; found: 6020 phrases; correct: 5503.\n",
      "accuracy:  92.97%; (non-O)\n",
      "accuracy:  98.60%; precision:  91.41%; recall:  92.61%; FB1:  92.01\n",
      "              LOC: precision:  94.92%; recall:  95.70%; FB1:  95.31  1852\n",
      "             MISC: precision:  85.82%; recall:  84.71%; FB1:  85.26  910\n",
      "              ORG: precision:  86.98%; recall:  89.19%; FB1:  88.07  1375\n",
      "              PER: precision:  93.89%; recall:  95.98%; FB1:  94.93  1883\n",
      "Epoch 18, Loss: 0.003682805795291312, time: 133.74659514427185s\n",
      "validation error: \n",
      "processed 51362 tokens with 5942 phrases; found: 6058 phrases; correct: 5526.\n",
      "accuracy:  93.25%; (non-O)\n",
      "accuracy:  98.63%; precision:  91.22%; recall:  93.00%; FB1:  92.10\n",
      "              LOC: precision:  94.57%; recall:  96.62%; FB1:  95.58  1877\n",
      "             MISC: precision:  84.57%; recall:  86.23%; FB1:  85.39  940\n",
      "              ORG: precision:  87.42%; recall:  88.07%; FB1:  87.74  1351\n",
      "              PER: precision:  93.92%; recall:  96.36%; FB1:  95.12  1890\n",
      "Epoch 19, Loss: 0.003912460297711236, time: 155.75212383270264s\n",
      "validation error: \n",
      "processed 51362 tokens with 5942 phrases; found: 6046 phrases; correct: 5518.\n",
      "accuracy:  93.14%; (non-O)\n",
      "accuracy:  98.63%; precision:  91.27%; recall:  92.86%; FB1:  92.06\n",
      "              LOC: precision:  94.93%; recall:  95.81%; FB1:  95.37  1854\n",
      "             MISC: precision:  83.76%; recall:  86.12%; FB1:  84.92  948\n",
      "              ORG: precision:  88.31%; recall:  87.92%; FB1:  88.12  1335\n",
      "              PER: precision:  93.50%; recall:  96.91%; FB1:  95.17  1909\n",
      "Epoch 20, Loss: 0.0027471507286959836, time: 69.84025812149048s\n",
      "validation error: \n",
      "processed 51362 tokens with 5942 phrases; found: 6044 phrases; correct: 5541.\n",
      "accuracy:  93.47%; (non-O)\n",
      "accuracy:  98.67%; precision:  91.68%; recall:  93.25%; FB1:  92.46\n",
      "              LOC: precision:  94.80%; recall:  96.30%; FB1:  95.54  1866\n",
      "             MISC: precision:  85.01%; recall:  86.12%; FB1:  85.56  934\n",
      "              ORG: precision:  88.16%; recall:  89.41%; FB1:  88.78  1360\n",
      "              PER: precision:  94.43%; recall:  96.58%; FB1:  95.49  1884\n"
     ]
    }
   ],
   "source": [
    "class BiLSTMNER(nn.Module):\n",
    "    def __init__(self,hidden_dim, output_dim, num_layers, dropout):\n",
    "        super(BiLSTMNER, self).__init__()\n",
    "        self.embedding = my_embedding_layer\n",
    "        self.capital_layer = nn.Embedding(num_embeddings=5,embedding_dim=20,padding_idx=4)\n",
    "        self.bilstm = nn.LSTM(input_size=120, hidden_size=hidden_dim, num_layers=num_layers, \n",
    "                              batch_first=True, bidirectional=True)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.linear = nn.Linear(hidden_dim * 2, output_dim,dtype=torch.float32)\n",
    "        self.elu = nn.ELU()\n",
    "        self.classifier = nn.Linear(output_dim, num_tags,dtype=torch.float32)  # num_tags is the number of unique NER tags\n",
    "\n",
    "    def forward(self, x, capital):\n",
    "        x = self.embedding(x.int())\n",
    "        capital = self.capital_layer(capital.int())\n",
    "        x = torch.cat([x, capital], dim=2)\n",
    "        x, _ = self.bilstm(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.linear(x)\n",
    "        x = self.elu(x)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "#initialize \n",
    "num_tags = 9\n",
    "model = BiLSTMNER(256,128, 1, 0.33) \n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "loss_function = nn.CrossEntropyLoss(ignore_index=9)\n",
    "\n",
    "\n",
    "#training\n",
    "num_epochs = 20\n",
    "print('start training')\n",
    "for epoch in range(num_epochs):\n",
    "    start_time = time.time()\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        inputs, capitals ,targets = batch\n",
    "        outputs = model(inputs, capitals)\n",
    "        batch_size = inputs.size()[-1]    \n",
    "        #From the instruction of CrossEntropy, we need to change the format of outputs \n",
    "        loss = loss_function(outputs.permute(0,2,1), targets) \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    end_time = time.time()\n",
    "    print(f'Epoch {epoch + 1}, Loss: {total_loss / len(train_loader)}, time: {end_time-start_time}s')\n",
    "    print('validation error: ')\n",
    "    precision, recall, f1 = eval(model, val_loader)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1179,
   "id": "3671eede-fdb9-4a00-acfd-ff09fc178fb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SAVE THE MODEL\n",
    "torch.save(model.state_dict(), 'task2.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1176,
   "id": "a86253e2-2de6-41f4-a251-3f915e01904c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation: precision = 91.67769688947716, recall = 93.25143049478291, f1 = 92.45786751209747\n"
     ]
    }
   ],
   "source": [
    "print(f\"Validation: precision = {precision}, recall = {recall}, f1 = {f1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1172,
   "id": "aa760197-d7e1-45c9-b705-83023df28a2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'O',\n",
       " 1: 'B-PER',\n",
       " 2: 'I-PER',\n",
       " 3: 'B-ORG',\n",
       " 4: 'I-ORG',\n",
       " 5: 'B-LOC',\n",
       " 6: 'I-LOC',\n",
       " 7: 'B-MISC',\n",
       " 8: 'I-MISC',\n",
       " 9: '<PAD>'}"
      ]
     },
     "execution_count": 1172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ner_tags = {'O': 0, 'B-PER': 1, 'I-PER': 2, 'B-ORG': 3, 'I-ORG': 4, 'B-LOC': 5, 'I-LOC': 6, 'B-MISC': 7, 'I-MISC': 8, '<PAD>':9}\n",
    "\n",
    "reversed_ner_tags = {value: key for key, value in ner_tags.items()}\n",
    "reversed_ner_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1177,
   "id": "da98ae87-9a15-4979-9451-b3b0827b8d33",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#evaluation\n",
    "def eval(model, loader):\n",
    "    model.eval()\n",
    "    all_preds, all_labels = [], []\n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            inputs, capitals ,targets = batch\n",
    "            #get rid of paddings on targets\n",
    "            label_unpad = targets\n",
    "            mask = label_unpad != 9\n",
    "            label_unpad = label_unpad[mask]\n",
    "            \n",
    "            outputs = model(inputs,capitals)\n",
    "            _, preds = torch.max(outputs, -1)\n",
    "            #get rid of paddings on pred\n",
    "            preds = preds[mask]\n",
    "            \n",
    "            preds_converted = [reversed_ner_tags[elem.item()] for elem in preds]\n",
    "            targets_converted = [reversed_ner_tags[elem.item()] for elem in label_unpad]\n",
    "            all_preds.extend(preds_converted)\n",
    "            all_labels.extend(targets_converted)\n",
    "    # all_preds = list(chain.from_iterable(all_preds))\n",
    "    # all_labels = list(chain.from_iterable(all_labels))\n",
    "    # all_labels = torch.cat(all_labels)\n",
    "    # all_preds = itertools.chain(*all_preds)    \n",
    "    # all_labels =itertools.chain(*all_labels)\n",
    "    result = evaluate(all_labels, all_preds,verbose=True)\n",
    "    precision, recall, f1 = result[0], result[1],result[2]\n",
    "    return precision, recall, f1\n",
    "\n",
    "# print('Test: ')\n",
    "# precision, recall, f1 = eval(model, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1178,
   "id": "a4544ca0-6c98-4818-94d3-f8e54681af90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test: precision = 91.67769688947716, recall = 93.25143049478291, f1 = 92.45786751209747\n"
     ]
    }
   ],
   "source": [
    "print(f\"Test: precision = {precision}, recall = {recall}, f1 = {f1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e432b29-ef93-4308-8814-3a6d87bc583c",
   "metadata": {},
   "source": [
    "### Solution for task2 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2ebbac6-cad4-4be4-9801-371601159426",
   "metadata": {},
   "source": [
    "1. Hyperparameters:\n",
    "\n",
    "- embedding_dim = 100\n",
    "- hidden_dim = 256 \n",
    "- output_dim = 128\n",
    "- num_layers = 1\n",
    "- dropout = 0.33\n",
    "- optimizer learning rate= 0.001\n",
    "- ignore_index = 9\n",
    "- batch_size = 64\n",
    "2. Solution:\n",
    "    At first, I loaded the glove embedding and convert it into two arrays. One records all the indices and the other one records the 100-d embeddings for all the tokens. Secondly, since the glove is not case-sensitive, I tried to divide tokens into 4 cases (0: lowercase 1: some uppercases 2: all uppercases 3: lowercase and uppercase are the same). So, I added a new list to the dataset. Thirdly, I mapped all the tokens into indices in the glove embedding. So, I added one more list to the dataset. Forthly, I created a new customized dataset that each batch contains (glove_ids, capitalize, ner_tag). And similiar to the task, I padded all of them while creating the dataloaders. To be notified, I padded 9 to the ner_tag since it is a number that has not been used. I padded the capitalize with 4, which is not used either. Fifthly, I threw the batches into the model, which has the similar structure to the task 1. However, I added one more embedding layer such that the feature capitalize will be converted into 20-d vector and be added to the original 100-d layer. So, the input will become a 120-d vector. Through elu, dropout, and one more linear layer, it model will predict the name entity for each token in samples.\n",
    "3. Questions and answers:\n",
    "- What is the precision, recall, and F1 score on the validation data?\n",
    "- precision = 91.67769688947716, recall = 93.25143049478291, f1 = 92.45786751209747\r",
    "- What are the precision, recall, and F1 score on the test data?\n",
    "- precision = 91.67769688947716, recall = 93.25143049478291, f1 = 92.45786751209747\r",
    "- BiLSTM with Glove Embeddings outperforms the model without. Can you provide a rationale for this?\n",
    "- At first, the glove is a bigger vocab than the word2idx, so it will map less unknown words. Secondly, since I added a new embedding layer, the model can better capture whether the word has been capitalized. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efb1b419-bcac-4398-8320-a1abcee2d298",
   "metadata": {},
   "source": [
    "## Task3: Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1180,
   "id": "a59ca3b4-efd7-462b-9f1a-adfee090c2e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([12, 13]), tensor([5, 0]))\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    \n",
    "    def __init__(self,data):\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        input = torch.tensor(self.data[index]['input_ids'], dtype=torch.long ) \n",
    "        target = torch.tensor(self.data[index]['ner_tags'], dtype=torch.long)\n",
    "        \n",
    "        return input, target\n",
    "\n",
    "# Create an instance of the CustomDataset\n",
    "dataset_train = CustomDataset(dataset['train'])\n",
    "dataset_test = CustomDataset(dataset['test'])\n",
    "dataset_val = CustomDataset(dataset['validation'])\n",
    "\n",
    "# Example: Accessing a single sample\n",
    "print(dataset_train[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1181,
   "id": "27013d20-7ff4-49ed-8a30-0d6a49c50c82",
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_collate(batch):\n",
    "    input_ids, label = zip(*batch)\n",
    "    padded_label = pad_sequence(label, batch_first=True, padding_value=9 )\n",
    "    padded_input_ids = pad_sequence(input_ids, batch_first=True, padding_value=0 )\n",
    "    return padded_input_ids, padded_label\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1182,
   "id": "4433783e-8624-41d8-bf2d-a358675907b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "train_loader = DataLoader(dataset_train, batch_size=batch_size, collate_fn= custom_collate, shuffle=True)\n",
    "test_loader = DataLoader(dataset_test, batch_size=batch_size, collate_fn= custom_collate, shuffle=False)\n",
    "val_loader = DataLoader(dataset_val, batch_size=batch_size, collate_fn= custom_collate, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1183,
   "id": "87802c59-8911-4184-b2dc-44a09b0d268c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# inspect the trainloader \n",
    "for batch in train_loader:\n",
    "    inputs, labels = batch\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1184,
   "id": "002e7560-2f20-4467-85f4-a67f86e07de2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class TransformerNERModel(nn.Module):\n",
    "    def __init__(self, vocab_size, tag_vocab_size, embed_size=128, num_heads=8, max_seq_length=128, ff_dim=128, num_encoder_layers=6,\n",
    "                dropout=0.33):\n",
    "        super(TransformerNERModel, self).__init__()\n",
    "        \n",
    "        # Token embedding layer\n",
    "        self.embedding = TokenEmbedding(vocab_size, embed_size)\n",
    "        \n",
    "        # Positional encoding\n",
    "        self.positional_encoder = PositionalEncoding(emb_size= embed_size, maxlen=max_seq_length)\n",
    "        \n",
    "        # Transformer Encoder\n",
    "        self.transformer_encoder = nn.TransformerEncoder(\n",
    "            nn.TransformerEncoderLayer(\n",
    "                d_model=embed_size,\n",
    "                nhead=num_heads,\n",
    "                dim_feedforward=ff_dim,\n",
    "                batch_first=True\n",
    "            ),\n",
    "            num_layers=num_encoder_layers,\n",
    "        )\n",
    "        \n",
    "        # Linear layer for classification\n",
    "        self.fc = nn.Linear(embed_size, tag_vocab_size)\n",
    "    \n",
    "    def forward(self, src, src_padding_mask):\n",
    "        # Token embedding\n",
    "        x = self.embedding(src)\n",
    "        \n",
    "        # Add positional encoding\n",
    "        x = self.positional_encoder(x)\n",
    "        \n",
    "        # Transformer encoder\n",
    "        x = self.transformer_encoder(x, src_key_padding_mask=src_padding_mask)\n",
    "        \n",
    "        # Final linear layer for classification\n",
    "        x = self.fc(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self,\n",
    "                 emb_size: int,\n",
    "                 dropout: float =0.33,\n",
    "                 maxlen: int = 5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        den = torch.exp(- torch.arange(0, emb_size, 2)* math.log(10000) / emb_size)\n",
    "        pos = torch.arange(0, maxlen).reshape(maxlen, 1)\n",
    "        pos_embedding = torch.zeros((maxlen, emb_size))\n",
    "        pos_embedding[:, 0::2] = torch.sin(pos * den)\n",
    "        pos_embedding[:, 1::2] = torch.cos(pos * den)\n",
    "        pos_embedding = pos_embedding.unsqueeze(-2)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.register_buffer('pos_embedding', pos_embedding)\n",
    "\n",
    "    def forward(self, token_embedding):\n",
    "        return self.dropout(token_embedding + self.pos_embedding[:token_embedding.size(0), :])\n",
    "\n",
    "class TokenEmbedding(nn.Module):\n",
    "    def __init__(self, vocab_size: int, emb_size):\n",
    "        super(TokenEmbedding, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, emb_size)\n",
    "        self.emb_size = emb_size\n",
    "\n",
    "    def forward(self, tokens):\n",
    "        return self.embedding(tokens.long()) * math.sqrt(self.emb_size)\n",
    "\n",
    "# Initialize the model\n",
    "vocab_size = max(word2idx.values())+1# Your vocabulary size\n",
    "tag_vocab_size = 9 # Your tag vocabulary size\n",
    "model = TransformerNERModel(vocab_size, tag_vocab_size)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=9)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1187,
   "id": "9ecea604-b9bd-4843-8275-778901e6a8a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/20] - Loss: 0.5256\n",
      "validation error: \n",
      "processed 51362 tokens with 5942 phrases; found: 3827 phrases; correct: 1857.\n",
      "accuracy:  29.40%; (non-O)\n",
      "accuracy:  87.05%; precision:  48.52%; recall:  31.25%; FB1:  38.02\n",
      "              LOC: precision:  58.47%; recall:  52.04%; FB1:  55.07  1635\n",
      "             MISC: precision:  64.33%; recall:  23.86%; FB1:  34.81  342\n",
      "              ORG: precision:  53.95%; recall:  17.30%; FB1:  26.20  430\n",
      "              PER: precision:  31.62%; recall:  24.38%; FB1:  27.53  1420\n",
      "Epoch [2/20] - Loss: 0.4477\n",
      "validation error: \n",
      "processed 51362 tokens with 5942 phrases; found: 3723 phrases; correct: 2158.\n",
      "accuracy:  34.28%; (non-O)\n",
      "accuracy:  88.53%; precision:  57.96%; recall:  36.32%; FB1:  44.66\n",
      "              LOC: precision:  75.40%; recall:  54.38%; FB1:  63.19  1325\n",
      "             MISC: precision:  72.88%; recall:  41.97%; FB1:  53.27  531\n",
      "              ORG: precision:  48.84%; recall:  29.83%; FB1:  37.04  819\n",
      "              PER: precision:  35.50%; recall:  20.20%; FB1:  25.74  1048\n",
      "Epoch [3/20] - Loss: 0.3974\n",
      "validation error: \n",
      "processed 51362 tokens with 5942 phrases; found: 4277 phrases; correct: 2363.\n",
      "accuracy:  39.20%; (non-O)\n",
      "accuracy:  89.35%; precision:  55.25%; recall:  39.77%; FB1:  46.25\n",
      "              LOC: precision:  80.41%; recall:  57.43%; FB1:  67.01  1312\n",
      "             MISC: precision:  67.03%; recall:  53.15%; FB1:  59.29  731\n",
      "              ORG: precision:  51.54%; recall:  31.25%; FB1:  38.90  813\n",
      "              PER: precision:  28.08%; recall:  21.66%; FB1:  24.46  1421\n",
      "Epoch [4/20] - Loss: 0.3614\n",
      "validation error: \n",
      "processed 51362 tokens with 5942 phrases; found: 5168 phrases; correct: 2721.\n",
      "accuracy:  44.73%; (non-O)\n",
      "accuracy:  89.73%; precision:  52.65%; recall:  45.79%; FB1:  48.98\n",
      "              LOC: precision:  78.47%; recall:  61.89%; FB1:  69.20  1449\n",
      "             MISC: precision:  67.09%; recall:  57.92%; FB1:  62.17  796\n",
      "              ORG: precision:  55.21%; recall:  34.75%; FB1:  42.65  844\n",
      "              PER: precision:  28.09%; recall:  31.70%; FB1:  29.79  2079\n",
      "Epoch [5/20] - Loss: 0.3321\n",
      "validation error: \n",
      "processed 51362 tokens with 5942 phrases; found: 5498 phrases; correct: 2887.\n",
      "accuracy:  49.01%; (non-O)\n",
      "accuracy:  90.45%; precision:  52.51%; recall:  48.59%; FB1:  50.47\n",
      "              LOC: precision:  82.54%; recall:  61.51%; FB1:  70.49  1369\n",
      "             MISC: precision:  69.62%; recall:  57.92%; FB1:  63.23  767\n",
      "              ORG: precision:  45.41%; recall:  47.58%; FB1:  46.47  1405\n",
      "              PER: precision:  29.89%; recall:  31.76%; FB1:  30.80  1957\n",
      "Epoch [6/20] - Loss: 0.3095\n",
      "validation error: \n",
      "processed 51362 tokens with 5942 phrases; found: 5344 phrases; correct: 2915.\n",
      "accuracy:  49.69%; (non-O)\n",
      "accuracy:  90.79%; precision:  54.55%; recall:  49.06%; FB1:  51.66\n",
      "              LOC: precision:  76.19%; recall:  65.16%; FB1:  70.25  1571\n",
      "             MISC: precision:  66.55%; recall:  61.50%; FB1:  63.92  852\n",
      "              ORG: precision:  50.60%; recall:  37.96%; FB1:  43.37  1006\n",
      "              PER: precision:  33.52%; recall:  34.85%; FB1:  34.18  1915\n",
      "Epoch [7/20] - Loss: 0.2901\n",
      "validation error: \n",
      "processed 51362 tokens with 5942 phrases; found: 5740 phrases; correct: 2861.\n",
      "accuracy:  49.97%; (non-O)\n",
      "accuracy:  90.56%; precision:  49.84%; recall:  48.15%; FB1:  48.98\n",
      "              LOC: precision:  77.28%; recall:  66.09%; FB1:  71.24  1571\n",
      "             MISC: precision:  64.77%; recall:  61.61%; FB1:  63.15  877\n",
      "              ORG: precision:  50.54%; recall:  31.17%; FB1:  38.56  827\n",
      "              PER: precision:  26.82%; recall:  35.88%; FB1:  30.69  2465\n",
      "Epoch [8/20] - Loss: 0.2772\n",
      "validation error: \n",
      "processed 51362 tokens with 5942 phrases; found: 5131 phrases; correct: 3051.\n",
      "accuracy:  51.10%; (non-O)\n",
      "accuracy:  91.32%; precision:  59.46%; recall:  51.35%; FB1:  55.11\n",
      "              LOC: precision:  82.48%; recall:  66.14%; FB1:  73.41  1473\n",
      "             MISC: precision:  68.18%; recall:  63.45%; FB1:  65.73  858\n",
      "              ORG: precision:  54.11%; recall:  48.10%; FB1:  50.93  1192\n",
      "              PER: precision:  37.69%; recall:  32.90%; FB1:  35.13  1608\n",
      "Epoch [9/20] - Loss: 0.2623\n",
      "validation error: \n",
      "processed 51362 tokens with 5942 phrases; found: 5718 phrases; correct: 3226.\n",
      "accuracy:  54.99%; (non-O)\n",
      "accuracy:  91.36%; precision:  56.42%; recall:  54.29%; FB1:  55.33\n",
      "              LOC: precision:  85.86%; recall:  62.17%; FB1:  72.12  1330\n",
      "             MISC: precision:  71.46%; recall:  64.10%; FB1:  67.58  827\n",
      "              ORG: precision:  49.14%; recall:  53.09%; FB1:  51.04  1449\n",
      "              PER: precision:  36.98%; recall:  42.40%; FB1:  39.50  2112\n",
      "Epoch [10/20] - Loss: 0.2519\n",
      "validation error: \n",
      "processed 51362 tokens with 5942 phrases; found: 5262 phrases; correct: 3086.\n",
      "accuracy:  52.47%; (non-O)\n",
      "accuracy:  91.59%; precision:  58.65%; recall:  51.94%; FB1:  55.09\n",
      "              LOC: precision:  83.68%; recall:  65.05%; FB1:  73.20  1428\n",
      "             MISC: precision:  72.64%; recall:  66.81%; FB1:  69.60  848\n",
      "              ORG: precision:  54.62%; recall:  52.42%; FB1:  53.50  1287\n",
      "              PER: precision:  33.67%; recall:  31.05%; FB1:  32.31  1699\n",
      "Epoch [11/20] - Loss: 0.2380\n",
      "validation error: \n",
      "processed 51362 tokens with 5942 phrases; found: 5797 phrases; correct: 3229.\n",
      "accuracy:  54.00%; (non-O)\n",
      "accuracy:  91.25%; precision:  55.70%; recall:  54.34%; FB1:  55.01\n",
      "              LOC: precision:  81.90%; recall:  66.03%; FB1:  73.12  1481\n",
      "             MISC: precision:  70.90%; recall:  66.59%; FB1:  68.68  866\n",
      "              ORG: precision:  50.79%; recall:  52.42%; FB1:  51.60  1384\n",
      "              PER: precision:  33.83%; recall:  37.95%; FB1:  35.77  2066\n",
      "Epoch [12/20] - Loss: 0.2313\n",
      "validation error: \n",
      "processed 51362 tokens with 5942 phrases; found: 5443 phrases; correct: 3330.\n",
      "accuracy:  54.59%; (non-O)\n",
      "accuracy:  91.61%; precision:  61.18%; recall:  56.04%; FB1:  58.50\n",
      "              LOC: precision:  81.57%; recall:  68.43%; FB1:  74.42  1541\n",
      "             MISC: precision:  72.21%; recall:  65.94%; FB1:  68.93  842\n",
      "              ORG: precision:  54.06%; recall:  54.06%; FB1:  54.06  1341\n",
      "              PER: precision:  43.05%; recall:  40.17%; FB1:  41.56  1719\n",
      "Epoch [13/20] - Loss: 0.2247\n",
      "validation error: \n",
      "processed 51362 tokens with 5942 phrases; found: 5448 phrases; correct: 3308.\n",
      "accuracy:  55.54%; (non-O)\n",
      "accuracy:  91.77%; precision:  60.72%; recall:  55.67%; FB1:  58.09\n",
      "              LOC: precision:  80.15%; recall:  68.37%; FB1:  73.80  1567\n",
      "             MISC: precision:  75.03%; recall:  69.41%; FB1:  72.11  853\n",
      "              ORG: precision:  55.00%; recall:  45.56%; FB1:  49.84  1111\n",
      "              PER: precision:  41.78%; recall:  43.49%; FB1:  42.62  1917\n",
      "Epoch [14/20] - Loss: 0.2153\n",
      "validation error: \n",
      "processed 51362 tokens with 5942 phrases; found: 5521 phrases; correct: 3226.\n",
      "accuracy:  55.07%; (non-O)\n",
      "accuracy:  91.79%; precision:  58.43%; recall:  54.29%; FB1:  56.29\n",
      "              LOC: precision:  79.81%; recall:  68.86%; FB1:  73.93  1585\n",
      "             MISC: precision:  70.17%; recall:  68.11%; FB1:  69.12  895\n",
      "              ORG: precision:  54.26%; recall:  46.53%; FB1:  50.10  1150\n",
      "              PER: precision:  37.49%; recall:  38.49%; FB1:  37.99  1891\n",
      "Epoch [15/20] - Loss: 0.2109\n",
      "validation error: \n",
      "processed 51362 tokens with 5942 phrases; found: 5244 phrases; correct: 3283.\n",
      "accuracy:  54.82%; (non-O)\n",
      "accuracy:  91.86%; precision:  62.60%; recall:  55.25%; FB1:  58.70\n",
      "              LOC: precision:  79.68%; recall:  68.32%; FB1:  73.56  1575\n",
      "             MISC: precision:  71.86%; recall:  67.03%; FB1:  69.36  860\n",
      "              ORG: precision:  55.08%; recall:  50.11%; FB1:  52.48  1220\n",
      "              PER: precision:  46.44%; recall:  40.07%; FB1:  43.02  1589\n",
      "Epoch [16/20] - Loss: 0.2058\n",
      "validation error: \n",
      "processed 51362 tokens with 5942 phrases; found: 5285 phrases; correct: 3317.\n",
      "accuracy:  55.93%; (non-O)\n",
      "accuracy:  92.03%; precision:  62.76%; recall:  55.82%; FB1:  59.09\n",
      "              LOC: precision:  82.97%; recall:  68.43%; FB1:  75.00  1515\n",
      "             MISC: precision:  72.47%; recall:  69.09%; FB1:  70.74  879\n",
      "              ORG: precision:  57.44%; recall:  49.81%; FB1:  53.35  1163\n",
      "              PER: precision:  43.69%; recall:  40.99%; FB1:  42.30  1728\n",
      "Epoch [17/20] - Loss: 0.2001\n",
      "validation error: \n",
      "processed 51362 tokens with 5942 phrases; found: 5265 phrases; correct: 3308.\n",
      "accuracy:  55.90%; (non-O)\n",
      "accuracy:  91.97%; precision:  62.83%; recall:  55.67%; FB1:  59.03\n",
      "              LOC: precision:  84.92%; recall:  65.92%; FB1:  74.23  1426\n",
      "             MISC: precision:  68.81%; recall:  71.80%; FB1:  70.28  962\n",
      "              ORG: precision:  55.74%; recall:  50.71%; FB1:  53.10  1220\n",
      "              PER: precision:  45.56%; recall:  40.99%; FB1:  43.16  1657\n",
      "Epoch [18/20] - Loss: 0.1967\n",
      "validation error: \n",
      "processed 51362 tokens with 5942 phrases; found: 5334 phrases; correct: 3472.\n",
      "accuracy:  57.34%; (non-O)\n",
      "accuracy:  92.18%; precision:  65.09%; recall:  58.43%; FB1:  61.58\n",
      "              LOC: precision:  83.97%; recall:  67.88%; FB1:  75.08  1485\n",
      "             MISC: precision:  75.44%; recall:  69.96%; FB1:  72.59  855\n",
      "              ORG: precision:  56.91%; recall:  56.23%; FB1:  56.56  1325\n",
      "              PER: precision:  49.49%; recall:  44.84%; FB1:  47.05  1669\n",
      "Epoch [19/20] - Loss: 0.1907\n",
      "validation error: \n",
      "processed 51362 tokens with 5942 phrases; found: 5652 phrases; correct: 3506.\n",
      "accuracy:  57.48%; (non-O)\n",
      "accuracy:  91.84%; precision:  62.03%; recall:  59.00%; FB1:  60.48\n",
      "              LOC: precision:  79.36%; recall:  71.15%; FB1:  75.03  1647\n",
      "             MISC: precision:  78.36%; recall:  70.28%; FB1:  74.10  827\n",
      "              ORG: precision:  57.29%; recall:  50.41%; FB1:  53.63  1180\n",
      "              PER: precision:  43.79%; recall:  47.50%; FB1:  45.57  1998\n",
      "Epoch [20/20] - Loss: 0.1868\n",
      "validation error: \n",
      "processed 51362 tokens with 5942 phrases; found: 5710 phrases; correct: 3461.\n",
      "accuracy:  58.29%; (non-O)\n",
      "accuracy:  92.03%; precision:  60.61%; recall:  58.25%; FB1:  59.41\n",
      "              LOC: precision:  83.07%; recall:  70.50%; FB1:  76.27  1559\n",
      "             MISC: precision:  75.51%; recall:  68.87%; FB1:  72.04  841\n",
      "              ORG: precision:  57.02%; recall:  53.02%; FB1:  54.95  1247\n",
      "              PER: precision:  39.75%; recall:  44.52%; FB1:  42.00  2063\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "num_epochs = 25\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    \n",
    "    # Iterate over your training data in batches\n",
    "    for batch_idx, (inputs, targets) in enumerate(train_loader):\n",
    "        # Zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass + src_padding_mask\n",
    "        src_padding_mask = (inputs == 0).float()\n",
    "        outputs = model(inputs, src_padding_mask= src_padding_mask)  \n",
    "        \n",
    "        # Flatten the outputs and targets for the loss calculation\n",
    "        outputs = outputs.view(-1, 9)\n",
    "        targets = targets.view(-1)\n",
    "        \n",
    "        # Calculate the loss\n",
    "        loss = criterion(outputs, targets)\n",
    "        \n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    # Print the average loss for this epoch\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    print(f\"Epoch [{epoch + 1}/{num_epochs}] - Loss: {avg_loss:.4f}\")\n",
    "\n",
    "    print('validation error: ')\n",
    "    precision, recall, f1 = eval(model, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1191,
   "id": "9709d19a-47c8-4c89-9831-bad1ae12d7d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [22/25] - Loss: 0.1733\n",
      "validation error: \n",
      "processed 51362 tokens with 5942 phrases; found: 5506 phrases; correct: 3568.\n",
      "accuracy:  59.12%; (non-O)\n",
      "accuracy:  92.24%; precision:  64.80%; recall:  60.05%; FB1:  62.33\n",
      "              LOC: precision:  83.40%; recall:  69.73%; FB1:  75.96  1536\n",
      "             MISC: precision:  76.79%; recall:  71.04%; FB1:  73.80  853\n",
      "              ORG: precision:  57.69%; recall:  53.69%; FB1:  55.62  1248\n",
      "              PER: precision:  48.80%; recall:  49.51%; FB1:  49.15  1869\n",
      "Epoch [23/25] - Loss: 0.1700\n",
      "validation error: \n",
      "processed 51362 tokens with 5942 phrases; found: 5337 phrases; correct: 3540.\n",
      "accuracy:  56.79%; (non-O)\n",
      "accuracy:  92.03%; precision:  66.33%; recall:  59.58%; FB1:  62.77\n",
      "              LOC: precision:  84.71%; recall:  69.95%; FB1:  76.62  1517\n",
      "             MISC: precision:  76.77%; recall:  70.61%; FB1:  73.56  848\n",
      "              ORG: precision:  55.55%; recall:  57.87%; FB1:  56.68  1397\n",
      "              PER: precision:  52.57%; recall:  44.95%; FB1:  48.46  1575\n",
      "Epoch [24/25] - Loss: 0.1684\n",
      "validation error: \n",
      "processed 51362 tokens with 5942 phrases; found: 5823 phrases; correct: 3586.\n",
      "accuracy:  58.87%; (non-O)\n",
      "accuracy:  91.86%; precision:  61.58%; recall:  60.35%; FB1:  60.96\n",
      "              LOC: precision:  84.38%; recall:  69.41%; FB1:  76.16  1511\n",
      "             MISC: precision:  76.08%; recall:  68.66%; FB1:  72.18  832\n",
      "              ORG: precision:  54.89%; recall:  56.08%; FB1:  55.48  1370\n",
      "              PER: precision:  43.89%; recall:  50.27%; FB1:  46.86  2110\n"
     ]
    }
   ],
   "source": [
    "# 3 more epochs\n",
    "for epoch in range(3):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    \n",
    "    # Iterate over your training data in batches\n",
    "    for batch_idx, (inputs, targets) in enumerate(train_loader):\n",
    "        # Zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass + src_padding_mask\n",
    "        src_padding_mask = (inputs == 0).float()\n",
    "        outputs = model(inputs, src_padding_mask= src_padding_mask)  \n",
    "        \n",
    "        # Flatten the outputs and targets for the loss calculation\n",
    "        outputs = outputs.view(-1, 9)\n",
    "        targets = targets.view(-1)\n",
    "        \n",
    "        # Calculate the loss\n",
    "        loss = criterion(outputs, targets)\n",
    "        \n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    # Print the average loss for this epoch\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    print(f\"Epoch [{epoch+22}/{25}] - Loss: {avg_loss:.4f}\")\n",
    "\n",
    "    print('validation error: ')\n",
    "    precision, recall, f1 = eval(model, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1192,
   "id": "43ed734f-1bc5-4681-9feb-4971038506a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation: precision = 61.58337626652928, recall = 60.35005048805117, f1 = 60.96047598810029\n"
     ]
    }
   ],
   "source": [
    "print(f\"Validation: precision = {precision}, recall = {recall}, f1 = {f1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1193,
   "id": "ea342755-8805-4824-abf9-a837da311bc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SAVE THE MODEL\n",
    "torch.save(model.state_dict(), 'task3.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1186,
   "id": "4cc00915-153c-4fd7-9889-c4189ba7aad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#evaluation\n",
    "def eval(model, loader):\n",
    "    model.eval()\n",
    "    all_preds, all_labels = [], []\n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            inputs, targets = batch\n",
    "            #get rid of paddings on targets\n",
    "            label_unpad = targets\n",
    "            mask = label_unpad != 9\n",
    "            label_unpad = label_unpad[mask]\n",
    "            src_padding_mask = (inputs == 0).float()\n",
    "                # print('size match:', src_padding_mask.size() == inputs.size())\n",
    "            outputs = model(inputs,src_padding_mask=src_padding_mask)\n",
    "            _, preds = torch.max(outputs, -1)\n",
    "            #get rid of paddings on pred\n",
    "            preds = preds[mask]\n",
    "            \n",
    "            preds_converted = [reversed_ner_tags[elem.item()] for elem in preds]\n",
    "            targets_converted = [reversed_ner_tags[elem.item()] for elem in label_unpad]\n",
    "            all_preds.extend(preds_converted)\n",
    "            all_labels.extend(targets_converted)\n",
    "    # all_preds = list(chain.from_iterable(all_preds))\n",
    "    # all_labels = list(chain.from_iterable(all_labels))\n",
    "    # all_labels = torch.cat(all_labels)\n",
    "    # all_preds = itertools.chain(*all_preds)    \n",
    "    # all_labels =itertools.chain(*all_labels)\n",
    "    result = evaluate(all_labels, all_preds,verbose=True)\n",
    "    precision, recall, f1 = result[0], result[1],result[2]\n",
    "    return precision, recall, f1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1194,
   "id": "b5737f00-876c-4847-878e-6368623e9c22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test: \n",
      "processed 46435 tokens with 5648 phrases; found: 5360 phrases; correct: 2830.\n",
      "accuracy:  50.48%; (non-O)\n",
      "accuracy:  89.56%; precision:  52.80%; recall:  50.11%; FB1:  51.42\n",
      "              LOC: precision:  79.85%; recall:  64.15%; FB1:  71.14  1340\n",
      "             MISC: precision:  67.76%; recall:  61.97%; FB1:  64.73  642\n",
      "              ORG: precision:  49.84%; recall:  45.88%; FB1:  47.77  1529\n",
      "              PER: precision:  30.45%; recall:  34.82%; FB1:  32.49  1849\n"
     ]
    }
   ],
   "source": [
    "print('Test: ')\n",
    "precision, recall, f1 = eval(model, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1195,
   "id": "af3eea95-311f-41ab-a300-a3ed62de479c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test: precision = 52.79850746268657, recall = 50.106232294617556, f1 = 51.417151162790695\n"
     ]
    }
   ],
   "source": [
    "print(f\"Test: precision = {precision}, recall = {recall}, f1 = {f1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bcaf1a6-1e8a-4dac-8f75-1e8af86b5455",
   "metadata": {},
   "source": [
    "### Solution to task 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d6e3d84-c448-4906-ae09-32073f6ddc39",
   "metadata": {},
   "source": [
    "1. Hyperparameters:\n",
    "- embedding_dim = 100\n",
    "- hidden_dim = 256 \n",
    "- output_dim = 128\n",
    "- num_layers = 1\n",
    "- dropout = 0.33\n",
    "- optimizer learning rate= 0.001\n",
    "- ignore_index = 9\n",
    "- batch_size = 32\n",
    "2. Solution:\n",
    "    Same as task, we still use input_ids as the input. The dataloader will pad 0 to input and 9 to ner_tags. Next, the first layer of the model is an embedding layer, which convert each token into 128-d vector. The positional encoder is a self-attention layer which will generate a sequence as output. For src_padding_mask, it will identify all the padded values and get rid of their impact. Next, the bacthes will be thrown to the transformer encoder and a FFN to predict the results.\n",
    "3. Questions and answers:\n",
    "- What is the precision, recall, and F1 score on the validation data?\n",
    "- precision = 61.58337626652928, recall = 60.35005048805117, f1 = 60.96047598810029\n",
    "- What are the precision, recall, and F1 score on the test data?\n",
    "- precision = 52.79850746268657, recall = 50.106232294617556, f1 = 51.417151162790695\r",
    "- What is the reason behind the poor performance of the transformer?\n",
    "- At first, the transformer typically require big amout of data. Since the word2idx is too small, it cannot generalize well onto the new data. Secondly, the other problem of the small dataset is that the model will probably overfit."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
