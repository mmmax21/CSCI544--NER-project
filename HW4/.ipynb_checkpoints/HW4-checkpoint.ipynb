{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 828,
   "id": "fe949759",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "import datasets\n",
    "import torch\n",
    "import math\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, classification_report\n",
    "from torch.utils.data import TensorDataset\n",
    "import time\n",
    "from itertools import chain\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import torch.nn.functional as F\n",
    "import copy\n",
    "from torch.optim import lr_scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "7a9312a5-7da1-44f7-abad-174bb55393fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import math\n",
    "# this ensures that the current MacOS version is at least 12.3+\n",
    "print(torch.backends.mps.is_available())\n",
    "# this ensures that the current current PyTorch installation was built with MPS activated.\n",
    "print(torch.backends.mps.is_built())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "2e2c2eb4-6228-4f99-98d3-6da34fd4dd54",
   "metadata": {},
   "outputs": [],
   "source": [
    "dtype = torch.float\n",
    "device = torch.device(\"mps\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "id": "759b054e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = datasets.load_dataset(\"conll2003\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4739328e-15bc-4609-b0b9-8aac9c074e9b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f2e8eaf8",
   "metadata": {},
   "source": [
    "### Convert words/tokens to indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 683,
   "id": "f4850e59",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "from collections import Counter\n",
    "\n",
    "word_frequency = Counter(itertools.chain(*dataset['train']['tokens']))  # type: ignore\n",
    "\n",
    "# Remove words below threshold 3\n",
    "word_frequency = {\n",
    "    word: frequency\n",
    "    for word, frequency in word_frequency.items()\n",
    "    if frequency >= 3\n",
    "}\n",
    "\n",
    "word2idx = {\n",
    "    word: index\n",
    "    for index, word in enumerate(word_frequency.keys(), start=2)\n",
    "}\n",
    "\n",
    "word2idx['[PAD]'] = 0\n",
    "word2idx['[UNK]'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1005,
   "id": "d0b0c346-a2f9-458e-815a-244281124355",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '0',\n",
       " 'tokens': ['EU',\n",
       "  'rejects',\n",
       "  'German',\n",
       "  'call',\n",
       "  'to',\n",
       "  'boycott',\n",
       "  'British',\n",
       "  'lamb',\n",
       "  '.'],\n",
       " 'pos_tags': [22, 42, 16, 21, 35, 37, 16, 21, 7],\n",
       " 'chunk_tags': [11, 21, 11, 12, 21, 22, 11, 12, 0],\n",
       " 'ner_tags': [3, 0, 7, 0, 0, 0, 7, 0, 0],\n",
       " 'input_ids': [2, 1, 3, 4, 5, 6, 7, 8, 9],\n",
       " 'capitals': [2, 0, 1, 0, 0, 0, 1, 0, 3]}"
      ]
     },
     "execution_count": 1005,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1004,
   "id": "5b867dbb-a783-4076-ad35-37eb35e959a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['EU', 'rejects', 'German', 'call', 'to', 'boycott', 'British', 'lamb', '.']"
      ]
     },
     "execution_count": 1004,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_tokens = dataset['train'][0]['tokens']\n",
    "sample_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1025,
   "id": "c7e8fc07-73ae-43f5-872c-86bbac50cc01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'eu'"
      ]
     },
     "execution_count": 1025,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = 'EU'\n",
    "s = s.lower()\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1026,
   "id": "c1629568-d8dc-4a6b-a09a-40de7421d676",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36bed5590cea4062a0c63da305d481c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/14041 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1026], line 14\u001b[0m\n\u001b[1;32m     12\u001b[0m     sample[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mglove_ids\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m glove_ids\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m sample\n\u001b[0;32m---> 14\u001b[0m dataset \u001b[38;5;241m=\u001b[39m \u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert_word_to_glove_ids\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/torch-gpu/lib/python3.8/site-packages/datasets/dataset_dict.py:853\u001b[0m, in \u001b[0;36mDatasetDict.map\u001b[0;34m(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_names, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, desc)\u001b[0m\n\u001b[1;32m    850\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cache_file_names \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    851\u001b[0m     cache_file_names \u001b[38;5;241m=\u001b[39m {k: \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m}\n\u001b[1;32m    852\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m DatasetDict(\n\u001b[0;32m--> 853\u001b[0m     {\n\u001b[1;32m    854\u001b[0m         k: dataset\u001b[38;5;241m.\u001b[39mmap(\n\u001b[1;32m    855\u001b[0m             function\u001b[38;5;241m=\u001b[39mfunction,\n\u001b[1;32m    856\u001b[0m             with_indices\u001b[38;5;241m=\u001b[39mwith_indices,\n\u001b[1;32m    857\u001b[0m             with_rank\u001b[38;5;241m=\u001b[39mwith_rank,\n\u001b[1;32m    858\u001b[0m             input_columns\u001b[38;5;241m=\u001b[39minput_columns,\n\u001b[1;32m    859\u001b[0m             batched\u001b[38;5;241m=\u001b[39mbatched,\n\u001b[1;32m    860\u001b[0m             batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[1;32m    861\u001b[0m             drop_last_batch\u001b[38;5;241m=\u001b[39mdrop_last_batch,\n\u001b[1;32m    862\u001b[0m             remove_columns\u001b[38;5;241m=\u001b[39mremove_columns,\n\u001b[1;32m    863\u001b[0m             keep_in_memory\u001b[38;5;241m=\u001b[39mkeep_in_memory,\n\u001b[1;32m    864\u001b[0m             load_from_cache_file\u001b[38;5;241m=\u001b[39mload_from_cache_file,\n\u001b[1;32m    865\u001b[0m             cache_file_name\u001b[38;5;241m=\u001b[39mcache_file_names[k],\n\u001b[1;32m    866\u001b[0m             writer_batch_size\u001b[38;5;241m=\u001b[39mwriter_batch_size,\n\u001b[1;32m    867\u001b[0m             features\u001b[38;5;241m=\u001b[39mfeatures,\n\u001b[1;32m    868\u001b[0m             disable_nullable\u001b[38;5;241m=\u001b[39mdisable_nullable,\n\u001b[1;32m    869\u001b[0m             fn_kwargs\u001b[38;5;241m=\u001b[39mfn_kwargs,\n\u001b[1;32m    870\u001b[0m             num_proc\u001b[38;5;241m=\u001b[39mnum_proc,\n\u001b[1;32m    871\u001b[0m             desc\u001b[38;5;241m=\u001b[39mdesc,\n\u001b[1;32m    872\u001b[0m         )\n\u001b[1;32m    873\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m k, dataset \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems()\n\u001b[1;32m    874\u001b[0m     }\n\u001b[1;32m    875\u001b[0m )\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/torch-gpu/lib/python3.8/site-packages/datasets/dataset_dict.py:854\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    850\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cache_file_names \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    851\u001b[0m     cache_file_names \u001b[38;5;241m=\u001b[39m {k: \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m}\n\u001b[1;32m    852\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m DatasetDict(\n\u001b[1;32m    853\u001b[0m     {\n\u001b[0;32m--> 854\u001b[0m         k: \u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    855\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfunction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    856\u001b[0m \u001b[43m            \u001b[49m\u001b[43mwith_indices\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwith_indices\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    857\u001b[0m \u001b[43m            \u001b[49m\u001b[43mwith_rank\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwith_rank\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    858\u001b[0m \u001b[43m            \u001b[49m\u001b[43minput_columns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_columns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    859\u001b[0m \u001b[43m            \u001b[49m\u001b[43mbatched\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatched\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    860\u001b[0m \u001b[43m            \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    861\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdrop_last_batch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdrop_last_batch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    862\u001b[0m \u001b[43m            \u001b[49m\u001b[43mremove_columns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mremove_columns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    863\u001b[0m \u001b[43m            \u001b[49m\u001b[43mkeep_in_memory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeep_in_memory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    864\u001b[0m \u001b[43m            \u001b[49m\u001b[43mload_from_cache_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mload_from_cache_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    865\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcache_file_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_file_names\u001b[49m\u001b[43m[\u001b[49m\u001b[43mk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    866\u001b[0m \u001b[43m            \u001b[49m\u001b[43mwriter_batch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwriter_batch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    867\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfeatures\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    868\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdisable_nullable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdisable_nullable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    869\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfn_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfn_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    870\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnum_proc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_proc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    871\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdesc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdesc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    872\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    873\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m k, dataset \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems()\n\u001b[1;32m    874\u001b[0m     }\n\u001b[1;32m    875\u001b[0m )\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/torch-gpu/lib/python3.8/site-packages/datasets/arrow_dataset.py:592\u001b[0m, in \u001b[0;36mtransmit_tasks.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    590\u001b[0m     \u001b[38;5;28mself\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mself\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    591\u001b[0m \u001b[38;5;66;03m# apply actual function\u001b[39;00m\n\u001b[0;32m--> 592\u001b[0m out: Union[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDatasetDict\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    593\u001b[0m datasets: List[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(out\u001b[38;5;241m.\u001b[39mvalues()) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(out, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m [out]\n\u001b[1;32m    594\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m dataset \u001b[38;5;129;01min\u001b[39;00m datasets:\n\u001b[1;32m    595\u001b[0m     \u001b[38;5;66;03m# Remove task templates if a column mapping of the template is no longer valid\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/torch-gpu/lib/python3.8/site-packages/datasets/arrow_dataset.py:557\u001b[0m, in \u001b[0;36mtransmit_format.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    550\u001b[0m self_format \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    551\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtype\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_type,\n\u001b[1;32m    552\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mformat_kwargs\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_kwargs,\n\u001b[1;32m    553\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_columns,\n\u001b[1;32m    554\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput_all_columns\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output_all_columns,\n\u001b[1;32m    555\u001b[0m }\n\u001b[1;32m    556\u001b[0m \u001b[38;5;66;03m# apply actual function\u001b[39;00m\n\u001b[0;32m--> 557\u001b[0m out: Union[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDatasetDict\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    558\u001b[0m datasets: List[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(out\u001b[38;5;241m.\u001b[39mvalues()) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(out, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m [out]\n\u001b[1;32m    559\u001b[0m \u001b[38;5;66;03m# re-apply format to the output\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/torch-gpu/lib/python3.8/site-packages/datasets/arrow_dataset.py:3097\u001b[0m, in \u001b[0;36mDataset.map\u001b[0;34m(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, suffix_template, new_fingerprint, desc)\u001b[0m\n\u001b[1;32m   3090\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m transformed_dataset \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3091\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m logging\u001b[38;5;241m.\u001b[39mtqdm(\n\u001b[1;32m   3092\u001b[0m         disable\u001b[38;5;241m=\u001b[39m\u001b[38;5;129;01mnot\u001b[39;00m logging\u001b[38;5;241m.\u001b[39mis_progress_bar_enabled(),\n\u001b[1;32m   3093\u001b[0m         unit\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m examples\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   3094\u001b[0m         total\u001b[38;5;241m=\u001b[39mpbar_total,\n\u001b[1;32m   3095\u001b[0m         desc\u001b[38;5;241m=\u001b[39mdesc \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMap\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   3096\u001b[0m     ) \u001b[38;5;28;01mas\u001b[39;00m pbar:\n\u001b[0;32m-> 3097\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m rank, done, content \u001b[38;5;129;01min\u001b[39;00m Dataset\u001b[38;5;241m.\u001b[39m_map_single(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mdataset_kwargs):\n\u001b[1;32m   3098\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m done:\n\u001b[1;32m   3099\u001b[0m                 shards_done \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/torch-gpu/lib/python3.8/site-packages/datasets/arrow_dataset.py:3450\u001b[0m, in \u001b[0;36mDataset._map_single\u001b[0;34m(shard, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, new_fingerprint, rank, offset)\u001b[0m\n\u001b[1;32m   3448\u001b[0m _time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m   3449\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, example \u001b[38;5;129;01min\u001b[39;00m shard_iterable:\n\u001b[0;32m-> 3450\u001b[0m     example \u001b[38;5;241m=\u001b[39m \u001b[43mapply_function_on_filtered_inputs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexample\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moffset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moffset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3451\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m update_data:\n\u001b[1;32m   3452\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/torch-gpu/lib/python3.8/site-packages/datasets/arrow_dataset.py:3353\u001b[0m, in \u001b[0;36mDataset._map_single.<locals>.apply_function_on_filtered_inputs\u001b[0;34m(pa_inputs, indices, check_same_num_examples, offset)\u001b[0m\n\u001b[1;32m   3351\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m with_rank:\n\u001b[1;32m   3352\u001b[0m     additional_args \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (rank,)\n\u001b[0;32m-> 3353\u001b[0m processed_inputs \u001b[38;5;241m=\u001b[39m \u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfn_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43madditional_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfn_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3354\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(processed_inputs, LazyDict):\n\u001b[1;32m   3355\u001b[0m     processed_inputs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m   3356\u001b[0m         k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m processed_inputs\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m processed_inputs\u001b[38;5;241m.\u001b[39mkeys_to_format\n\u001b[1;32m   3357\u001b[0m     }\n",
      "Cell \u001b[0;32mIn[1026], line 6\u001b[0m, in \u001b[0;36mconvert_word_to_glove_ids\u001b[0;34m(sample)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m tokens:\n\u001b[1;32m      5\u001b[0m     token \u001b[38;5;241m=\u001b[39m token\u001b[38;5;241m.\u001b[39mlower()\n\u001b[0;32m----> 6\u001b[0m     indices \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mwhere(\u001b[43mvocab_npa\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtoken\u001b[49m)\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m indices[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39msize \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m      8\u001b[0m         index \u001b[38;5;241m=\u001b[39m indices[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def convert_word_to_glove_ids(sample):\n",
    "    tokens = sample['tokens']\n",
    "    glove_ids =[]\n",
    "    for token in tokens:\n",
    "        token = token.lower()\n",
    "        indices = np.where(vocab_npa == token)\n",
    "        if indices[0].size > 0:\n",
    "            index = indices[0][0]\n",
    "        else:\n",
    "            index = 1\n",
    "        glove_ids.append(index)\n",
    "    sample['glove_ids'] = glove_ids\n",
    "    return sample\n",
    "dataset = dataset.map(convert_word_to_glove_ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1012,
   "id": "9d853cdd-4c5a-4126-8896-9ff80b280941",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '0',\n",
       " 'tokens': ['EU',\n",
       "  'rejects',\n",
       "  'German',\n",
       "  'call',\n",
       "  'to',\n",
       "  'boycott',\n",
       "  'British',\n",
       "  'lamb',\n",
       "  '.'],\n",
       " 'pos_tags': [22, 42, 16, 21, 35, 37, 16, 21, 7],\n",
       " 'chunk_tags': [11, 21, 11, 12, 21, 22, 11, 12, 0],\n",
       " 'ner_tags': [3, 0, 7, 0, 0, 0, 7, 0, 0],\n",
       " 'input_ids': [2, 1, 3, 4, 5, 6, 7, 8, 9],\n",
       " 'capitals': [2, 0, 1, 0, 0, 0, 1, 0, 3],\n",
       " 'glove_ids': [1, 7580, 1, 582, 6, 5262, 1, 10240, 4]}"
      ]
     },
     "execution_count": 1012,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 684,
   "id": "1910d653",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8128"
      ]
     },
     "execution_count": 684,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the vocab size\n",
    "vocab_size = max(word2idx.values())+1\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 689,
   "id": "0ff50b8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c5826c19686471186739c43db27835d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/14041 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b45f3b7732ac4be3a6d538cfd39f97d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3250 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7bd3d2c161d245348beefdabb3d21899",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3453 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def convert_word_to_id(sample):\n",
    "#Code to convert all tokens to their respective indexes\n",
    "#If the token is unknown, we set index of 1\n",
    "    input_ids = [ word2idx.get(token, 1) for token in sample['tokens'] ]\n",
    "\n",
    "    sample['input_ids'] = input_ids\n",
    "    return sample\n",
    "\n",
    "dataset = dataset.map(convert_word_to_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 691,
   "id": "ec7ecf74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '20',\n",
       " 'tokens': ['Rare',\n",
       "  'Hendrix',\n",
       "  'song',\n",
       "  'draft',\n",
       "  'sells',\n",
       "  'for',\n",
       "  'almost',\n",
       "  '$',\n",
       "  '17,000',\n",
       "  '.'],\n",
       " 'pos_tags': [22, 22, 21, 21, 42, 15, 30, 3, 11, 7],\n",
       " 'chunk_tags': [11, 12, 12, 12, 21, 13, 11, 12, 12, 0],\n",
       " 'ner_tags': [0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       " 'input_ids': [1, 225, 1, 226, 227, 63, 228, 229, 1, 9]}"
      ]
     },
     "execution_count": 691,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train'][20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 692,
   "id": "5360f2c0-bf75-4618-9282-a5401a769433",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.DataFrame(dataset['train']).drop(columns=['pos_tags', 'chunk_tags', 'id', 'tokens'])\n",
    "df_train.columns = ['label','input_ids']\n",
    "\n",
    "df_test = pd.DataFrame(dataset['test']).drop(columns=['pos_tags', 'chunk_tags', 'id', 'tokens'])\n",
    "df_test.columns = ['label','input_ids']\n",
    "\n",
    "df_val = pd.DataFrame(dataset['validation']).drop(columns=['pos_tags', 'chunk_tags', 'id', 'tokens'])\n",
    "df_val.columns = ['label','input_ids']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab935b85",
   "metadata": {},
   "source": [
    "### Padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "id": "f0e42ff3-55c5-47ee-a9cd-1a385cd7290f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([13, 14]), tensor([5, 0]))\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "# Create a custom Dataset class\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, dataframe):\n",
    "        self.data = dataframe\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        label = torch.tensor(self.data.loc[idx, \"label\"], dtype=torch.long)\n",
    "        input_ids = torch.tensor(self.data.loc[idx, \"input_ids\"], dtype=torch.long)\n",
    "\n",
    "        return input_ids, label\n",
    "\n",
    "# Create an instance of the CustomDataset\n",
    "dataset_train = CustomDataset(df_train)\n",
    "\n",
    "# Example: Accessing a single sample\n",
    "print(dataset_train[2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "id": "9733d6c5-271e-4ea8-b045-3375b2b1b5ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_collate(batch):\n",
    "    # Separate input sequences and labels\n",
    "    input_seqs, labels = zip(*batch)\n",
    "    \n",
    "    # Calculate the sequence lengths based on input sequences (assuming they have the same length as labels)\n",
    "    sequence_lengths = [len(seq) for seq in input_seqs]\n",
    "\n",
    "    # Sort input sequences and labels by sequence length (descending)\n",
    "    sorted_seqs_and_labels = sorted(zip(input_seqs, labels), key=lambda x: len(x[0]), reverse=True)\n",
    "    sorted_input_seqs, sorted_labels = zip(*sorted_seqs_and_labels)\n",
    "\n",
    "    # Pad input sequences to the maximum length within the batch\n",
    "    padded_input_seqs = pad_sequence(sorted_input_seqs, batch_first=True, padding_value=0)  # Use 0 as the padding value\n",
    "    padded_labels = pad_sequence(sorted_labels, batch_first=True, padding_value=0)  # Use 0 as the padding value\n",
    "\n",
    "    return padded_input_seqs, padded_labels\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdb31028",
   "metadata": {},
   "source": [
    "### Create dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 676,
   "id": "1fb5f65d-2664-4f5b-8796-b81f2321c889",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataloader_generator(df,shuffle):\n",
    "    dataset_from_df = CustomDataset(df)\n",
    "    batch_size = 64\n",
    "    dataloader = DataLoader(dataset_from_df, batch_size=batch_size, collate_fn=custom_collate, shuffle=shuffle)\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 693,
   "id": "2c110bd6-4d78-415b-8c1b-4ec53b5ddd38",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader  = dataloader_generator(df_train,shuffle=True)\n",
    "test_loader  = dataloader_generator(df_test,shuffle=False)\n",
    "val_loader  = dataloader_generator(df_val,shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 816,
   "id": "7554c09a-a936-4fbb-998f-a64414566ac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in val_loader:\n",
    "    input_val, target_val = batch\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d37b135",
   "metadata": {},
   "source": [
    "### Glove Embedding "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0a4787e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to load GloVe embeddings from a file\n",
    "def load_glove_embeddings(file_path):\n",
    "    embeddings_index = {}\n",
    "    with open(file_path, encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            coefs = np.asarray(values[1:], dtype=\"float32\")\n",
    "            embeddings_index[word] = coefs\n",
    "    return embeddings_index\n",
    "\n",
    "# Specify the path to your downloaded \"glove.6B.100d.txt\" file\n",
    "glove_file_path = \"glove.6B.100d\"\n",
    "\n",
    "# Load GloVe embeddings into memory\n",
    "glove_embeddings = load_glove_embeddings(glove_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3d9a65f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glove_embeddings['the'].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ed603a3",
   "metadata": {},
   "source": [
    "### Building the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "id": "05e32ef5-344d-4f13-8023-abf45527db64",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2023-11-06 16:54:57--  https://raw.githubusercontent.com/sighsmile/conlleval/master/conlleval.py\n",
      "正在解析主机 raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
      "正在连接 raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... 已连接。\n",
      "已发出 HTTP 请求，正在等待回应... 200 OK\n",
      "长度：7502 (7.3K) [text/plain]\n",
      "正在保存至: “conlleval.py.1”\n",
      "\n",
      "conlleval.py.1      100%[===================>]   7.33K  --.-KB/s  用时 0s        \n",
      "\n",
      "2023-11-06 16:54:57 (14.7 MB/s) - 已保存 “conlleval.py.1” [7502/7502])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://raw.githubusercontent.com/sighsmile/conlleval/master/conlleval.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "id": "05c1cda1-f710-42ac-a158-784280ed44b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from conlleval import evaluate "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 696,
   "id": "5b253ac4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start training\n",
      "Epoch 1, Loss: 0.23767325075512583, time: 45.66872692108154s\n",
      "validation error: \n",
      "processed 152266 tokens with 5942 phrases; found: 2289 phrases; correct: 1248.\n",
      "accuracy:  22.29%; (non-O)\n",
      "accuracy:  95.45%; precision:  54.52%; recall:  21.00%; FB1:  30.32\n",
      "              LOC: precision:  55.38%; recall:  38.65%; FB1:  45.53  1282\n",
      "             MISC: precision:  43.48%; recall:   2.17%; FB1:   4.13  46\n",
      "              ORG: precision:  40.45%; recall:   2.68%; FB1:   5.03  89\n",
      "              PER: precision:  55.28%; recall:  26.17%; FB1:  35.52  872\n",
      "Epoch 2, Loss: 0.11316225692968476, time: 45.273293256759644s\n",
      "validation error: \n",
      "processed 152266 tokens with 5942 phrases; found: 4274 phrases; correct: 2818.\n",
      "accuracy:  51.10%; (non-O)\n",
      "accuracy:  97.06%; precision:  65.93%; recall:  47.43%; FB1:  55.17\n",
      "              LOC: precision:  76.54%; recall:  57.38%; FB1:  65.59  1377\n",
      "             MISC: precision:  64.45%; recall:  35.79%; FB1:  46.03  512\n",
      "              ORG: precision:  51.53%; recall:  35.27%; FB1:  41.88  918\n",
      "              PER: precision:  65.51%; recall:  52.17%; FB1:  58.08  1467\n",
      "Epoch 3, Loss: 0.06862446303394708, time: 45.66086411476135s\n",
      "validation error: \n",
      "processed 152266 tokens with 5942 phrases; found: 5130 phrases; correct: 3664.\n",
      "accuracy:  65.16%; (non-O)\n",
      "accuracy:  97.83%; precision:  71.42%; recall:  61.66%; FB1:  66.18\n",
      "              LOC: precision:  79.64%; recall:  71.31%; FB1:  75.24  1645\n",
      "             MISC: precision:  70.03%; recall:  57.27%; FB1:  63.01  754\n",
      "              ORG: precision:  58.88%; recall:  50.19%; FB1:  54.19  1143\n",
      "              PER: precision:  72.61%; recall:  62.60%; FB1:  67.23  1588\n",
      "Epoch 4, Loss: 0.04689689292995767, time: 43.55336809158325s\n",
      "validation error: \n",
      "processed 152266 tokens with 5942 phrases; found: 5557 phrases; correct: 4049.\n",
      "accuracy:  71.89%; (non-O)\n",
      "accuracy:  98.10%; precision:  72.86%; recall:  68.14%; FB1:  70.42\n",
      "              LOC: precision:  85.22%; recall:  72.18%; FB1:  78.16  1556\n",
      "             MISC: precision:  84.78%; recall:  59.22%; FB1:  69.73  644\n",
      "              ORG: precision:  57.02%; recall:  61.82%; FB1:  59.32  1454\n",
      "              PER: precision:  70.84%; recall:  73.18%; FB1:  71.99  1903\n",
      "Epoch 5, Loss: 0.03328430595892397, time: 43.41973876953125s\n",
      "validation error: \n",
      "processed 152266 tokens with 5942 phrases; found: 5260 phrases; correct: 4152.\n",
      "accuracy:  73.16%; (non-O)\n",
      "accuracy:  98.32%; precision:  78.94%; recall:  69.88%; FB1:  74.13\n",
      "              LOC: precision:  87.73%; recall:  76.32%; FB1:  81.63  1598\n",
      "             MISC: precision:  82.20%; recall:  68.11%; FB1:  74.50  764\n",
      "              ORG: precision:  67.82%; recall:  61.30%; FB1:  64.39  1212\n",
      "              PER: precision:  77.11%; recall:  70.58%; FB1:  73.70  1686\n",
      "Epoch 6, Loss: 0.024703141157938674, time: 44.008224964141846s\n",
      "validation error: \n",
      "processed 152266 tokens with 5942 phrases; found: 5473 phrases; correct: 4314.\n",
      "accuracy:  75.58%; (non-O)\n",
      "accuracy:  98.41%; precision:  78.82%; recall:  72.60%; FB1:  75.58\n",
      "              LOC: precision:  84.69%; recall:  81.00%; FB1:  82.80  1757\n",
      "             MISC: precision:  80.86%; recall:  69.63%; FB1:  74.83  794\n",
      "              ORG: precision:  69.00%; recall:  65.40%; FB1:  67.15  1271\n",
      "              PER: precision:  79.16%; recall:  70.96%; FB1:  74.84  1651\n",
      "Epoch 7, Loss: 0.018182350913147356, time: 45.57333779335022s\n",
      "validation error: \n",
      "processed 152266 tokens with 5942 phrases; found: 5825 phrases; correct: 4505.\n",
      "accuracy:  78.95%; (non-O)\n",
      "accuracy:  98.43%; precision:  77.34%; recall:  75.82%; FB1:  76.57\n",
      "              LOC: precision:  82.18%; recall:  83.61%; FB1:  82.89  1869\n",
      "             MISC: precision:  80.17%; recall:  71.91%; FB1:  75.81  827\n",
      "              ORG: precision:  69.55%; recall:  67.11%; FB1:  68.31  1294\n",
      "              PER: precision:  76.62%; recall:  76.33%; FB1:  76.48  1835\n",
      "Epoch 8, Loss: 0.014101394826800309, time: 46.32741093635559s\n",
      "validation error: \n",
      "processed 152266 tokens with 5942 phrases; found: 5647 phrases; correct: 4439.\n",
      "accuracy:  77.80%; (non-O)\n",
      "accuracy:  98.48%; precision:  78.61%; recall:  74.71%; FB1:  76.61\n",
      "              LOC: precision:  84.20%; recall:  82.69%; FB1:  83.44  1804\n",
      "             MISC: precision:  82.21%; recall:  71.15%; FB1:  76.28  798\n",
      "              ORG: precision:  70.55%; recall:  66.82%; FB1:  68.63  1270\n",
      "              PER: precision:  77.07%; recall:  74.27%; FB1:  75.64  1775\n",
      "Epoch 9, Loss: 0.010744672634808177, time: 44.75534200668335s\n",
      "validation error: \n",
      "processed 152266 tokens with 5942 phrases; found: 5635 phrases; correct: 4456.\n",
      "accuracy:  77.37%; (non-O)\n",
      "accuracy:  98.47%; precision:  79.08%; recall:  74.99%; FB1:  76.98\n",
      "              LOC: precision:  86.94%; recall:  82.63%; FB1:  84.73  1746\n",
      "             MISC: precision:  78.71%; recall:  71.37%; FB1:  74.86  836\n",
      "              ORG: precision:  67.67%; recall:  68.83%; FB1:  68.24  1364\n",
      "              PER: precision:  80.34%; recall:  73.67%; FB1:  76.86  1689\n",
      "Epoch 10, Loss: 0.008262550728183916, time: 44.441893100738525s\n",
      "validation error: \n",
      "processed 152266 tokens with 5942 phrases; found: 5475 phrases; correct: 4369.\n",
      "accuracy:  76.45%; (non-O)\n",
      "accuracy:  98.44%; precision:  79.80%; recall:  73.53%; FB1:  76.53\n",
      "              LOC: precision:  87.75%; recall:  81.87%; FB1:  84.71  1714\n",
      "             MISC: precision:  82.21%; recall:  70.17%; FB1:  75.72  787\n",
      "              ORG: precision:  66.29%; recall:  69.50%; FB1:  67.86  1406\n",
      "              PER: precision:  82.02%; recall:  69.82%; FB1:  75.43  1568\n",
      "Epoch 11, Loss: 0.006700816870116713, time: 44.44866895675659s\n",
      "validation error: \n",
      "processed 152266 tokens with 5942 phrases; found: 6017 phrases; correct: 4536.\n",
      "accuracy:  79.53%; (non-O)\n",
      "accuracy:  98.37%; precision:  75.39%; recall:  76.34%; FB1:  75.86\n",
      "              LOC: precision:  88.56%; recall:  80.08%; FB1:  84.11  1661\n",
      "             MISC: precision:  81.80%; recall:  71.15%; FB1:  76.10  802\n",
      "              ORG: precision:  59.72%; recall:  72.18%; FB1:  65.36  1621\n",
      "              PER: precision:  74.55%; recall:  78.23%; FB1:  76.34  1933\n",
      "Epoch 12, Loss: 0.005611294611695815, time: 44.84120297431946s\n",
      "validation error: \n",
      "processed 152266 tokens with 5942 phrases; found: 5711 phrases; correct: 4464.\n",
      "accuracy:  78.03%; (non-O)\n",
      "accuracy:  98.46%; precision:  78.16%; recall:  75.13%; FB1:  76.62\n",
      "              LOC: precision:  88.48%; recall:  80.73%; FB1:  84.43  1676\n",
      "             MISC: precision:  79.50%; recall:  71.91%; FB1:  75.51  834\n",
      "              ORG: precision:  65.20%; recall:  68.46%; FB1:  66.79  1408\n",
      "              PER: precision:  78.08%; recall:  76.00%; FB1:  77.03  1793\n",
      "Epoch 13, Loss: 0.004643922152040019, time: 45.98033404350281s\n",
      "validation error: \n",
      "processed 152266 tokens with 5942 phrases; found: 5787 phrases; correct: 4540.\n",
      "accuracy:  79.26%; (non-O)\n",
      "accuracy:  98.51%; precision:  78.45%; recall:  76.41%; FB1:  77.41\n",
      "              LOC: precision:  87.25%; recall:  82.36%; FB1:  84.74  1734\n",
      "             MISC: precision:  81.21%; recall:  71.26%; FB1:  75.91  809\n",
      "              ORG: precision:  70.04%; recall:  70.25%; FB1:  70.14  1345\n",
      "              PER: precision:  75.20%; recall:  77.52%; FB1:  76.34  1899\n",
      "Epoch 14, Loss: 0.003888597339127128, time: 44.79966902732849s\n",
      "validation error: \n",
      "processed 152266 tokens with 5942 phrases; found: 5726 phrases; correct: 4526.\n",
      "accuracy:  78.81%; (non-O)\n",
      "accuracy:  98.52%; precision:  79.04%; recall:  76.17%; FB1:  77.58\n",
      "              LOC: precision:  89.74%; recall:  80.95%; FB1:  85.12  1657\n",
      "             MISC: precision:  80.99%; recall:  72.56%; FB1:  76.54  826\n",
      "              ORG: precision:  72.09%; recall:  67.79%; FB1:  69.87  1261\n",
      "              PER: precision:  73.71%; recall:  79.32%; FB1:  76.41  1982\n",
      "Epoch 15, Loss: 0.003382106341516853, time: 44.445590019226074s\n",
      "validation error: \n",
      "processed 152266 tokens with 5942 phrases; found: 5774 phrases; correct: 4534.\n",
      "accuracy:  78.60%; (non-O)\n",
      "accuracy:  98.48%; precision:  78.52%; recall:  76.30%; FB1:  77.40\n",
      "              LOC: precision:  84.38%; recall:  83.78%; FB1:  84.08  1824\n",
      "             MISC: precision:  79.04%; recall:  73.21%; FB1:  76.01  854\n",
      "              ORG: precision:  71.88%; recall:  69.20%; FB1:  70.52  1291\n",
      "              PER: precision:  77.12%; recall:  75.57%; FB1:  76.34  1805\n",
      "Epoch 16, Loss: 0.003079541970510036, time: 46.30193567276001s\n",
      "validation error: \n",
      "processed 152266 tokens with 5942 phrases; found: 5731 phrases; correct: 4491.\n",
      "accuracy:  78.28%; (non-O)\n",
      "accuracy:  98.47%; precision:  78.36%; recall:  75.58%; FB1:  76.95\n",
      "              LOC: precision:  89.98%; recall:  80.13%; FB1:  84.77  1636\n",
      "             MISC: precision:  75.25%; recall:  73.54%; FB1:  74.38  901\n",
      "              ORG: precision:  67.26%; recall:  70.47%; FB1:  68.83  1405\n",
      "              PER: precision:  78.03%; recall:  75.79%; FB1:  76.89  1789\n",
      "Epoch 17, Loss: 0.0028033381400746292, time: 45.66768169403076s\n",
      "validation error: \n",
      "processed 152266 tokens with 5942 phrases; found: 6027 phrases; correct: 4508.\n",
      "accuracy:  78.90%; (non-O)\n",
      "accuracy:  98.38%; precision:  74.80%; recall:  75.87%; FB1:  75.33\n",
      "              LOC: precision:  86.27%; recall:  81.06%; FB1:  83.58  1726\n",
      "             MISC: precision:  74.27%; recall:  72.02%; FB1:  73.13  894\n",
      "              ORG: precision:  59.29%; recall:  73.08%; FB1:  65.46  1653\n",
      "              PER: precision:  78.39%; recall:  74.65%; FB1:  76.47  1754\n",
      "Epoch 18, Loss: 0.002972540454423076, time: 52.05805730819702s\n",
      "validation error: \n",
      "processed 152266 tokens with 5942 phrases; found: 5769 phrases; correct: 4485.\n",
      "accuracy:  78.38%; (non-O)\n",
      "accuracy:  98.47%; precision:  77.74%; recall:  75.48%; FB1:  76.59\n",
      "              LOC: precision:  88.01%; recall:  82.31%; FB1:  85.06  1718\n",
      "             MISC: precision:  78.33%; recall:  72.13%; FB1:  75.10  849\n",
      "              ORG: precision:  65.83%; recall:  70.69%; FB1:  68.18  1440\n",
      "              PER: precision:  77.19%; recall:  73.83%; FB1:  75.47  1762\n",
      "Epoch 19, Loss: 0.0024553789689062714, time: 47.18321919441223s\n",
      "validation error: \n",
      "processed 152266 tokens with 5942 phrases; found: 5884 phrases; correct: 4578.\n",
      "accuracy:  79.81%; (non-O)\n",
      "accuracy:  98.49%; precision:  77.80%; recall:  77.04%; FB1:  77.42\n",
      "              LOC: precision:  83.70%; recall:  84.98%; FB1:  84.33  1865\n",
      "             MISC: precision:  80.02%; recall:  70.82%; FB1:  75.14  816\n",
      "              ORG: precision:  68.88%; recall:  69.50%; FB1:  69.19  1353\n",
      "              PER: precision:  77.41%; recall:  77.74%; FB1:  77.57  1850\n",
      "Epoch 20, Loss: 0.0023643454028140034, time: 47.58099699020386s\n",
      "validation error: \n",
      "processed 152266 tokens with 5942 phrases; found: 5620 phrases; correct: 4481.\n",
      "accuracy:  77.67%; (non-O)\n",
      "accuracy:  98.51%; precision:  79.73%; recall:  75.41%; FB1:  77.51\n",
      "              LOC: precision:  88.24%; recall:  82.47%; FB1:  85.26  1717\n",
      "             MISC: precision:  82.77%; recall:  69.31%; FB1:  75.44  772\n",
      "              ORG: precision:  67.11%; recall:  70.92%; FB1:  68.96  1417\n",
      "              PER: precision:  80.28%; recall:  74.70%; FB1:  77.39  1714\n"
     ]
    }
   ],
   "source": [
    "class BiLSTMNER(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, num_layers, dropout):\n",
    "        super(BiLSTMNER, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.bilstm = nn.LSTM(embedding_dim, hidden_dim, num_layers=num_layers, \n",
    "                              batch_first=True, bidirectional=True)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.linear = nn.Linear(hidden_dim * 2, output_dim)\n",
    "        self.elu = nn.ELU()\n",
    "        self.classifier = nn.Linear(output_dim, num_tags)  # num_tags is the number of unique NER tags\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x, _ = self.bilstm(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.linear(x)\n",
    "        x = self.elu(x)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "#initialize \n",
    "num_tags = 9\n",
    "vocab_size = max(word2idx.values())+1\n",
    "\n",
    "model = BiLSTMNER(vocab_size, 100, 256, 128, 1, 0.33) \n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "\n",
    "#training\n",
    "num_epochs = 20\n",
    "print('start training')\n",
    "for epoch in range(num_epochs):\n",
    "    start_time = time.time()\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        inputs, targets = batch\n",
    "        outputs = model(inputs)\n",
    "        batch_size = inputs.size()[-1]    \n",
    "        #From the instruction of CrossEntropy, we need to change the format of outputs \n",
    "        loss = loss_function(outputs.permute(0,2,1), targets) \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    end_time = time.time()\n",
    "    print(f'Epoch {epoch + 1}, Loss: {total_loss / len(train_loader)}, time: {end_time-start_time}s')\n",
    "    print('validation error: ')\n",
    "    precision, recall, f1 = eval(model, val_loader)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 469,
   "id": "fe8ad821-7c63-4cb0-85c1-4d6df4369cb2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100.0, 100.0, 100.0)"
      ]
     },
     "execution_count": 469,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#example of using evalute function\n",
    "pred = [['O', 'O', 'B-ORG', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'], ['O','O']]\n",
    "true =  [['O', 'O', 'B-ORG', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'], ['O','O']]\n",
    "evaluate(itertools.chain(*true),itertools.chain(*pred), verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 444,
   "id": "3d45cc5b-8183-41e7-95d2-f595cd24edea",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Example reversed_ner_tags dictionary\n",
    "reversed_ner_tags = {\n",
    "    0: 'O',\n",
    "    1: 'B-PER',\n",
    "    2: 'I-PER',\n",
    "    3: 'B-ORG',\n",
    "    4: 'I-ORG',\n",
    "    5: 'B-LOC',\n",
    "    6: 'I-LOC',\n",
    "    7: 'B-MISC',\n",
    "    8: 'I-MISC'\n",
    "}\n",
    "\n",
    "# Example tensor with shape (32, 36)\n",
    "tensor = torch.randint(0, 9, (32, 36))  # Random integers between 0 and 8\n",
    "\n",
    "# Map tensor elements using reversed_ner_tags\n",
    "mapped_tensor = [[reversed_ner_tags[item.item()] for item in row] for row in tensor]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "id": "06214e1e-da38-4c52-92aa-eeffcc6c7ded",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'O',\n",
       " 1: 'B-PER',\n",
       " 2: 'I-PER',\n",
       " 3: 'B-ORG',\n",
       " 4: 'I-ORG',\n",
       " 5: 'B-LOC',\n",
       " 6: 'I-LOC',\n",
       " 7: 'B-MISC',\n",
       " 8: 'I-MISC'}"
      ]
     },
     "execution_count": 398,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ner_tags = {'O': 0, 'B-PER': 1, 'I-PER': 2, 'B-ORG': 3, 'I-ORG': 4, 'B-LOC': 5, 'I-LOC': 6, 'B-MISC': 7, 'I-MISC': 8}\n",
    "\n",
    "reversed_ner_tags = {value: key for key, value in ner_tags.items()}\n",
    "reversed_ner_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 697,
   "id": "7c48799c-6972-4695-b643-e350cf5449bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test: \n",
      "processed 146937 tokens with 5648 phrases; found: 5235 phrases; correct: 3731.\n",
      "accuracy:  71.18%; (non-O)\n",
      "accuracy:  97.98%; precision:  71.27%; recall:  66.06%; FB1:  68.57\n",
      "              LOC: precision:  81.99%; recall:  73.68%; FB1:  77.61  1499\n",
      "             MISC: precision:  70.92%; recall:  61.82%; FB1:  66.06  612\n",
      "              ORG: precision:  60.65%; recall:  62.91%; FB1:  61.76  1723\n",
      "              PER: precision:  73.02%; recall:  63.27%; FB1:  67.79  1401\n"
     ]
    }
   ],
   "source": [
    "#evaluation\n",
    "def eval(model, loader):\n",
    "    model.eval()\n",
    "    all_preds, all_labels = [], []\n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            inputs, targets = batch\n",
    "            outputs = model(inputs)\n",
    "            _, preds = torch.max(outputs, -1)\n",
    "            preds_converted = [[reversed_ner_tags[item.item()] for item in row] for row in preds]\n",
    "            targets_converted = [[reversed_ner_tags[item.item()] for item in row] for row in targets]\n",
    "            all_preds.extend(preds_converted)\n",
    "            all_labels.extend(targets_converted)\n",
    "    # all_preds = list(chain.from_iterable(all_preds))\n",
    "    # all_labels = list(chain.from_iterable(all_labels))\n",
    "    # all_labels = torch.cat(all_labels)\n",
    "    all_preds = itertools.chain(*all_preds)    \n",
    "    all_labels =itertools.chain(*all_labels)\n",
    "    result = evaluate(all_labels, all_preds,verbose=True)\n",
    "    precision, recall, f1 = result[0], result[1],result[2]\n",
    "    return precision, recall, f1\n",
    "\n",
    "print('Test: ')\n",
    "precision, recall, f1 = eval(model, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ba6fdc8-8597-44e6-ab80-385f377a2d8d",
   "metadata": {},
   "source": [
    "## Task 2: Glove Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edb202b9-0835-4d9d-baf6-ec97a9e6ad06",
   "metadata": {},
   "source": [
    "### Load Glove and customize the layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 699,
   "id": "950dbbf5-c46c-49a7-b96e-5f97be43a208",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to load GloVe embeddings from a file\n",
    "def load_glove_embeddings(file_path):\n",
    "    embeddings_index = {}\n",
    "    with open(file_path, encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            coefs = np.asarray(values[1:], dtype=\"float32\")\n",
    "            embeddings_index[word] = coefs\n",
    "    return embeddings_index\n",
    "\n",
    "# Specify the path to your downloaded \"glove.6B.100d.txt\" file\n",
    "glove_file_path = \"glove.6B.100d\"\n",
    "\n",
    "# Load GloVe embeddings into memory\n",
    "glove_embeddings = load_glove_embeddings(glove_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 831,
   "id": "934b5706-5dec-498d-9f3f-59366de8f3eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert glove into a layer\n",
    "vocab,embeddings = [],[]\n",
    "with open('glove.6B.100d',encoding=\"utf-8\") as fi:\n",
    "    full_content = fi.read().strip().split('\\n')\n",
    "for i in range(len(full_content)):\n",
    "    i_word = full_content[i].split(' ')[0]\n",
    "    i_embeddings = [float(val) for val in full_content[i].split(' ')[1:]]\n",
    "    vocab.append(i_word)\n",
    "    embeddings.append(i_embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 832,
   "id": "ae3b857c-1bda-40a4-9d00-c2cc8a83847c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<pad>' '<unk>' 'the' ',' '.' 'of' 'to' 'and' 'in' 'a']\n",
      "(400002, 100)\n"
     ]
    }
   ],
   "source": [
    "vocab_npa = np.array(vocab)\n",
    "embs_npa = np.array(embeddings)\n",
    "\n",
    "#insert '<pad>' and '<unk>' tokens at start of vocab_npa.\n",
    "vocab_npa = np.insert(vocab_npa, 0, '<pad>')\n",
    "vocab_npa = np.insert(vocab_npa, 1, '<unk>')\n",
    "print(vocab_npa[:10])\n",
    "\n",
    "pad_emb_npa = np.zeros((1,embs_npa.shape[1]))   #embedding for '<pad>' token.\n",
    "unk_emb_npa = np.mean(embs_npa,axis=0,keepdims=True)    #embedding for '<unk>' token.\n",
    "\n",
    "#insert embeddings for pad and unk tokens at top of embs_npa.\n",
    "embs_npa = np.vstack((pad_emb_npa,unk_emb_npa,embs_npa))\n",
    "print(embs_npa.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 988,
   "id": "7507cc9f-63de-4d8b-a9c8-a848bd59c380",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['<pad>', '<unk>', 'the', ',', '.', 'of', 'to', 'and', 'in', 'a'],\n",
       "      dtype='<U68')"
      ]
     },
     "execution_count": 988,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 877,
   "id": "22fbbef0-211c-408a-b8af-e8ab64d282dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([400002, 100])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "my_embedding_layer = torch.nn.Embedding.from_pretrained(torch.from_numpy(embs_npa).float(),freeze=True)\n",
    "\n",
    "assert my_embedding_layer.weight.shape == embs_npa.shape\n",
    "print(my_embedding_layer.weight.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1c9e037-fb69-4a54-bd30-974c5d05d41b",
   "metadata": {},
   "source": [
    "### Make Glove case-sensitive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 858,
   "id": "fca5158d-21c9-4eee-9500-d8da43d28fa6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4886c4d5aff9468793ddde6b0f1bc634",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/14041 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "afd6a197c12b4f349a8b8890a51df08a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3250 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5fdd47f2bcc457b96b30c8e68d82d77",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3453 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#add features to the dataloader \n",
    "#case 0: lower case - no uppercase\n",
    "#case 1: first word is uppercase\n",
    "#case 2: whole word is uppeercase\n",
    "#case 3: others: e.g. \",\"\n",
    "def capital_case(word):\n",
    "    if word.islower():\n",
    "        return 0\n",
    "    elif word.isupper():\n",
    "        return 2\n",
    "    elif word.istitle():\n",
    "        return 1\n",
    "    else: return 3\n",
    "\n",
    "def convert_word_to_capital_case(sample):\n",
    "    capitals = [capital_case(word) for word in sample['tokens'] ]\n",
    "    sample['capitals'] =capitals\n",
    "    return sample \n",
    "\n",
    "dataset = dataset.map(convert_word_to_capital_case)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1023,
   "id": "8e2eb2d5-dbff-4330-b6f3-4656fdabb82c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '2',\n",
       " 'tokens': ['BRUSSELS', '1996-08-22'],\n",
       " 'pos_tags': [22, 11],\n",
       " 'chunk_tags': [11, 12],\n",
       " 'ner_tags': [5, 0],\n",
       " 'input_ids': [12, 13],\n",
       " 'capitals': [2, 3],\n",
       " 'glove_ids': [1, 1]}"
      ]
     },
     "execution_count": 1023,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train'][2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de631f84-db7f-4c05-adac-d14e17453b36",
   "metadata": {},
   "source": [
    "### Padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1013,
   "id": "e4517d97-f5a7-4e7c-bbf1-248a90ee29b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([3, 0, 7, 0, 0, 0, 7, 0, 0]), tensor([    1,  7580,     1,   582,     6,  5262,     1, 10240,     4]), tensor([2, 0, 1, 0, 0, 0, 1, 0, 3]))\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "# Create a custom Dataset class\n",
    "class CustomDataset(Dataset):\n",
    "    \n",
    "    def __init__(self,data):\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        label = torch.tensor(self.data[index]['ner_tags'], dtype=torch.long ) \n",
    "        glove_ids = torch.tensor(self.data[index]['glove_ids'], dtype=torch.long)\n",
    "        capital = torch.tensor(self.data[index]['capitals'], dtype=torch.long)\n",
    "        \n",
    "        return label, glove_ids, capital\n",
    "\n",
    "# Create an instance of the CustomDataset\n",
    "dataset_train = CustomDataset(dataset['train'])\n",
    "dataset_test = CustomDataset(dataset['test'])\n",
    "dataset_val = CustomDataset(dataset['validation'])\n",
    "\n",
    "# Example: Accessing a single sample\n",
    "print(dataset_train[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1022,
   "id": "8b182b13-e9e2-4bc1-9082-7b29e295a081",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([    1,    11,  2045,     6,     2,     1,     1,    11, 11915,   448,\n",
       "            1,     1,    18,    15,     1,  2036,   191,   989,     1,    27,\n",
       "          254,    70,    75,     1,   209,     2,  2442,  3242,    17, 14349,\n",
       "            4])"
      ]
     },
     "execution_count": 1022,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_train[4][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1014,
   "id": "7ca68deb-5d45-411a-8d74-adca3b7b1a76",
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_collate(batch):\n",
    "    label, glove_ids, capital = zip(*batch)\n",
    "    padded_label = pad_sequence(label, batch_first=True, padding_value=9 )\n",
    "    padded_glove_ids = pad_sequence(glove_ids, batch_first=True, padding_value=0 )\n",
    "    padded_capital = pad_sequence(capital, batch_first=True, padding_value=4 )\n",
    "    return padded_glove_ids, padded_capital, padded_label\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1015,
   "id": "6a53c811-7114-4fbf-87de-810107a7eaa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "train_loader = DataLoader(dataset_train, batch_size=batch_size, collate_fn= custom_collate, shuffle=True)\n",
    "test_loader = DataLoader(dataset_test, batch_size=batch_size, collate_fn= custom_collate, shuffle=True)\n",
    "val_loader = DataLoader(dataset_val, batch_size=batch_size, collate_fn= custom_collate, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1017,
   "id": "32a2a985-9678-4920-a873-1ce100897623",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start training\n",
      "Epoch 1, Loss: 0.3167437568883961, time: 54.3510799407959s\n",
      "validation error: \n",
      "processed 51362 tokens with 5942 phrases; found: 5751 phrases; correct: 3344.\n",
      "accuracy:  61.20%; (non-O)\n",
      "accuracy:  92.58%; precision:  58.15%; recall:  56.28%; FB1:  57.20\n",
      "              LOC: precision:  75.47%; recall:  56.45%; FB1:  64.59  1374\n",
      "             MISC: precision:  37.79%; recall:  49.35%; FB1:  42.80  1204\n",
      "              ORG: precision:  43.88%; recall:  49.22%; FB1:  46.40  1504\n",
      "              PER: precision:  71.42%; recall:  64.71%; FB1:  67.90  1669\n",
      "Epoch 2, Loss: 0.19918248247448567, time: 58.02853608131409s\n",
      "validation error: \n",
      "processed 51362 tokens with 5942 phrases; found: 5573 phrases; correct: 3700.\n",
      "accuracy:  65.80%; (non-O)\n",
      "accuracy:  93.57%; precision:  66.39%; recall:  62.27%; FB1:  64.26\n",
      "              LOC: precision:  72.64%; recall:  64.89%; FB1:  68.55  1641\n",
      "             MISC: precision:  56.85%; recall:  36.44%; FB1:  44.42  591\n",
      "              ORG: precision:  58.49%; recall:  52.13%; FB1:  55.13  1195\n",
      "              PER: precision:  68.64%; recall:  79.97%; FB1:  73.87  2146\n",
      "Epoch 3, Loss: 0.1721863564197186, time: 56.914159059524536s\n",
      "validation error: \n",
      "processed 51362 tokens with 5942 phrases; found: 6041 phrases; correct: 3992.\n",
      "accuracy:  70.45%; (non-O)\n",
      "accuracy:  94.04%; precision:  66.08%; recall:  67.18%; FB1:  66.63\n",
      "              LOC: precision:  68.44%; recall:  71.53%; FB1:  69.95  1920\n",
      "             MISC: precision:  49.60%; recall:  46.96%; FB1:  48.25  873\n",
      "              ORG: precision:  61.68%; recall:  56.52%; FB1:  58.99  1229\n",
      "              PER: precision:  73.65%; recall:  80.73%; FB1:  77.03  2019\n",
      "Epoch 4, Loss: 0.15504371209371334, time: 53.443016052246094s\n",
      "validation error: \n",
      "processed 51362 tokens with 5942 phrases; found: 5771 phrases; correct: 3971.\n",
      "accuracy:  70.02%; (non-O)\n",
      "accuracy:  94.25%; precision:  68.81%; recall:  66.83%; FB1:  67.81\n",
      "              LOC: precision:  80.26%; recall:  63.75%; FB1:  71.06  1459\n",
      "             MISC: precision:  56.27%; recall:  49.13%; FB1:  52.46  805\n",
      "              ORG: precision:  53.54%; recall:  69.87%; FB1:  60.63  1750\n",
      "              PER: precision:  80.25%; recall:  76.55%; FB1:  78.36  1757\n",
      "Epoch 5, Loss: 0.13889475695466397, time: 53.354209899902344s\n",
      "validation error: \n",
      "processed 51362 tokens with 5942 phrases; found: 5854 phrases; correct: 4043.\n",
      "accuracy:  71.15%; (non-O)\n",
      "accuracy:  94.41%; precision:  69.06%; recall:  68.04%; FB1:  68.55\n",
      "              LOC: precision:  77.84%; recall:  66.74%; FB1:  71.86  1575\n",
      "             MISC: precision:  57.69%; recall:  45.55%; FB1:  50.91  728\n",
      "              ORG: precision:  54.86%; recall:  68.23%; FB1:  60.82  1668\n",
      "              PER: precision:  78.70%; recall:  80.46%; FB1:  79.57  1883\n",
      "Epoch 6, Loss: 0.12770459241740795, time: 54.70113492012024s\n",
      "validation error: \n",
      "processed 51362 tokens with 5942 phrases; found: 5579 phrases; correct: 4048.\n",
      "accuracy:  71.57%; (non-O)\n",
      "accuracy:  94.66%; precision:  72.56%; recall:  68.13%; FB1:  70.27\n",
      "              LOC: precision:  76.20%; recall:  68.48%; FB1:  72.13  1651\n",
      "             MISC: precision:  54.09%; recall:  54.56%; FB1:  54.32  930\n",
      "              ORG: precision:  70.70%; recall:  59.21%; FB1:  64.45  1123\n",
      "              PER: precision:  79.63%; recall:  81.05%; FB1:  80.33  1875\n",
      "Epoch 7, Loss: 0.11482244869163748, time: 53.673200845718384s\n",
      "validation error: \n",
      "processed 51362 tokens with 5942 phrases; found: 6006 phrases; correct: 4189.\n",
      "accuracy:  74.30%; (non-O)\n",
      "accuracy:  94.68%; precision:  69.75%; recall:  70.50%; FB1:  70.12\n",
      "              LOC: precision:  78.01%; recall:  68.75%; FB1:  73.09  1619\n",
      "             MISC: precision:  50.71%; recall:  58.35%; FB1:  54.26  1061\n",
      "              ORG: precision:  60.94%; recall:  69.80%; FB1:  65.07  1536\n",
      "              PER: precision:  81.12%; recall:  78.83%; FB1:  79.96  1790\n",
      "Epoch 8, Loss: 0.10442655107562254, time: 53.10446619987488s\n",
      "validation error: \n",
      "processed 51362 tokens with 5942 phrases; found: 5888 phrases; correct: 4205.\n",
      "accuracy:  74.29%; (non-O)\n",
      "accuracy:  94.86%; precision:  71.42%; recall:  70.77%; FB1:  71.09\n",
      "              LOC: precision:  77.45%; recall:  68.81%; FB1:  72.87  1632\n",
      "             MISC: precision:  52.43%; recall:  57.38%; FB1:  54.79  1009\n",
      "              ORG: precision:  66.30%; recall:  67.49%; FB1:  66.89  1365\n",
      "              PER: precision:  80.07%; recall:  81.81%; FB1:  80.93  1882\n",
      "Epoch 9, Loss: 0.09293861978672496, time: 55.52683782577515s\n",
      "validation error: \n",
      "processed 51362 tokens with 5942 phrases; found: 5942 phrases; correct: 4231.\n",
      "accuracy:  74.39%; (non-O)\n",
      "accuracy:  94.84%; precision:  71.20%; recall:  71.20%; FB1:  71.20\n",
      "              LOC: precision:  76.33%; recall:  72.51%; FB1:  74.37  1745\n",
      "             MISC: precision:  51.43%; recall:  60.52%; FB1:  55.61  1085\n",
      "              ORG: precision:  68.67%; recall:  65.70%; FB1:  67.15  1283\n",
      "              PER: precision:  79.83%; recall:  79.26%; FB1:  79.54  1829\n",
      "Epoch 10, Loss: 0.08278213284383602, time: 53.66984295845032s\n",
      "validation error: \n",
      "processed 51362 tokens with 5942 phrases; found: 5983 phrases; correct: 4207.\n",
      "accuracy:  73.85%; (non-O)\n",
      "accuracy:  94.75%; precision:  70.32%; recall:  70.80%; FB1:  70.56\n",
      "              LOC: precision:  81.38%; recall:  67.83%; FB1:  73.99  1531\n",
      "             MISC: precision:  51.53%; recall:  58.35%; FB1:  54.73  1044\n",
      "              ORG: precision:  58.83%; recall:  74.27%; FB1:  65.66  1693\n",
      "              PER: precision:  83.21%; recall:  77.47%; FB1:  80.24  1715\n",
      "Epoch 11, Loss: 0.07185822271727077, time: 53.016961097717285s\n",
      "validation error: \n",
      "processed 51362 tokens with 5942 phrases; found: 5944 phrases; correct: 4250.\n",
      "accuracy:  74.29%; (non-O)\n",
      "accuracy:  94.85%; precision:  71.50%; recall:  71.52%; FB1:  71.51\n",
      "              LOC: precision:  78.35%; recall:  70.11%; FB1:  74.00  1644\n",
      "             MISC: precision:  54.53%; recall:  60.09%; FB1:  57.17  1016\n",
      "              ORG: precision:  66.18%; recall:  67.26%; FB1:  66.72  1363\n",
      "              PER: precision:  78.40%; recall:  81.76%; FB1:  80.04  1921\n",
      "Epoch 12, Loss: 0.06342245704135889, time: 53.908353090286255s\n",
      "validation error: \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1017], line 56\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtotal_loss\u001b[38;5;250m \u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mlen\u001b[39m(train_loader)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, time: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mend_time\u001b[38;5;241m-\u001b[39mstart_time\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124ms\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvalidation error: \u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 56\u001b[0m precision, recall, f1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43meval\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[978], line 13\u001b[0m, in \u001b[0;36meval\u001b[0;34m(model, loader)\u001b[0m\n\u001b[1;32m     10\u001b[0m mask \u001b[38;5;241m=\u001b[39m label_unpad \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m9\u001b[39m\n\u001b[1;32m     11\u001b[0m label_unpad \u001b[38;5;241m=\u001b[39m label_unpad[mask]\n\u001b[0;32m---> 13\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43mcapitals\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m _, preds \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmax(outputs, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m#get rid of paddings on pred\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/torch-gpu/lib/python3.8/site-packages/torch/nn/modules/module.py:1519\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1517\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1518\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1519\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/torch-gpu/lib/python3.8/site-packages/torch/nn/modules/module.py:1528\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1525\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1527\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1528\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1530\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1531\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[1017], line 17\u001b[0m, in \u001b[0;36mBiLSTMNER.forward\u001b[0;34m(self, x, capital)\u001b[0m\n\u001b[1;32m     15\u001b[0m capital \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcapital_layer(capital\u001b[38;5;241m.\u001b[39mint())\n\u001b[1;32m     16\u001b[0m x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([x, capital], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m---> 17\u001b[0m x, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbilstm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(x)\n\u001b[1;32m     19\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlinear(x)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/torch-gpu/lib/python3.8/site-packages/torch/nn/modules/module.py:1519\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1517\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1518\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1519\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/torch-gpu/lib/python3.8/site-packages/torch/nn/modules/module.py:1528\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1525\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1527\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1528\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1530\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1531\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/torch-gpu/lib/python3.8/site-packages/torch/nn/modules/rnn.py:881\u001b[0m, in \u001b[0;36mLSTM.forward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    878\u001b[0m         hx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpermute_hidden(hx, sorted_indices)\n\u001b[1;32m    880\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m batch_sizes \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 881\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43m_VF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlstm\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_flat_weights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_layers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    882\u001b[0m \u001b[43m                      \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbidirectional\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_first\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    883\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    884\u001b[0m     result \u001b[38;5;241m=\u001b[39m _VF\u001b[38;5;241m.\u001b[39mlstm(\u001b[38;5;28minput\u001b[39m, batch_sizes, hx, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flat_weights, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias,\n\u001b[1;32m    885\u001b[0m                       \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_layers, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbidirectional)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "class BiLSTMNER(nn.Module):\n",
    "    def __init__(self,hidden_dim, output_dim, num_layers, dropout):\n",
    "        super(BiLSTMNER, self).__init__()\n",
    "        self.embedding = my_embedding_layer\n",
    "        self.capital_layer = nn.Embedding(num_embeddings=5,embedding_dim=20,padding_idx=4)\n",
    "        self.bilstm = nn.LSTM(input_size=120, hidden_size=hidden_dim, num_layers=num_layers, \n",
    "                              batch_first=True, bidirectional=True)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.linear = nn.Linear(hidden_dim * 2, output_dim,dtype=torch.float32)\n",
    "        self.elu = nn.ELU()\n",
    "        self.classifier = nn.Linear(output_dim, num_tags,dtype=torch.float32)  # num_tags is the number of unique NER tags\n",
    "\n",
    "    def forward(self, x, capital):\n",
    "        x = self.embedding(x.int())\n",
    "        capital = self.capital_layer(capital.int())\n",
    "        x = torch.cat([x, capital], dim=2)\n",
    "        x, _ = self.bilstm(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.linear(x)\n",
    "        x = self.elu(x)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "#initialize \n",
    "num_tags = 9\n",
    "# vocab_size = embs_npa.shape[0]\n",
    "# embedding_dim = embs_npa.shape[1]\n",
    "# embedding_dim= 120\n",
    "model = BiLSTMNER(256,128, 1, 0.33) \n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "loss_function = nn.CrossEntropyLoss(ignore_index=9)\n",
    "\n",
    "\n",
    "#training\n",
    "num_epochs = 30\n",
    "print('start training')\n",
    "for epoch in range(num_epochs):\n",
    "    start_time = time.time()\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        inputs, capitals ,targets = batch\n",
    "        outputs = model(inputs, capitals)\n",
    "        batch_size = inputs.size()[-1]    \n",
    "        #From the instruction of CrossEntropy, we need to change the format of outputs \n",
    "        loss = loss_function(outputs.permute(0,2,1), targets) \n",
    "        # loss = loss_function(outputs.view(-1, 9), targets.view(-1).long()) \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    end_time = time.time()\n",
    "    print(f'Epoch {epoch + 1}, Loss: {total_loss / len(train_loader)}, time: {end_time-start_time}s')\n",
    "    print('validation error: ')\n",
    "    precision, recall, f1 = eval(model, val_loader)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 909,
   "id": "aa760197-d7e1-45c9-b705-83023df28a2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'O',\n",
       " 1: 'B-PER',\n",
       " 2: 'I-PER',\n",
       " 3: 'B-ORG',\n",
       " 4: 'I-ORG',\n",
       " 5: 'B-LOC',\n",
       " 6: 'I-LOC',\n",
       " 7: 'B-MISC',\n",
       " 8: 'I-MISC',\n",
       " 9: '<PAD>'}"
      ]
     },
     "execution_count": 909,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ner_tags = {'O': 0, 'B-PER': 1, 'I-PER': 2, 'B-ORG': 3, 'I-ORG': 4, 'B-LOC': 5, 'I-LOC': 6, 'B-MISC': 7, 'I-MISC': 8, '<PAD>':9}\n",
    "\n",
    "reversed_ner_tags = {value: key for key, value in ner_tags.items()}\n",
    "reversed_ner_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 966,
   "id": "58c5fcf7-520d-41d7-bedd-06918a069619",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "all_preds, all_labels = [], []\n",
    "with torch.no_grad():\n",
    "    for batch in val_loader:\n",
    "        inputs, capitals ,targets = batch\n",
    "        #get rid of paddings on targets\n",
    "        label_unpad = targets\n",
    "        mask = label_unpad != 9\n",
    "        label_unpad = label_unpad[mask]\n",
    "        \n",
    "        outputs = model(inputs,capitals)\n",
    "        _, preds = torch.max(outputs, -1)\n",
    "        #get rid of paddings on pred\n",
    "        preds = preds[mask]\n",
    "        preds_converted = [reversed_ner_tags[elem.item()] for elem in preds]\n",
    "        targets_converted = [reversed_ner_tags[elem.item()] for elem in label_unpad]\n",
    "        all_preds.extend(preds_converted)\n",
    "        all_labels.extend(targets_converted)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 975,
   "id": "06b44871-5f1c-4e48-a438-986b40e8dd38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed 51362 tokens with 5942 phrases; found: 5591 phrases; correct: 3379.\n",
      "accuracy:  61.84%; (non-O)\n",
      "accuracy:  92.99%; precision:  60.44%; recall:  56.87%; FB1:  58.60\n",
      "              LOC: precision:  83.75%; recall:  49.65%; FB1:  62.34  1089\n",
      "             MISC: precision:  50.00%; recall:  36.44%; FB1:  42.16  672\n",
      "              ORG: precision:  49.43%; recall:  51.60%; FB1:  50.49  1400\n",
      "              PER: precision:  59.22%; recall:  78.12%; FB1:  67.37  2430\n"
     ]
    }
   ],
   "source": [
    "result = evaluate(all_labels, all_preds,verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 973,
   "id": "3c41b082-0963-4a6a-be79-bde952f829d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "51362"
      ]
     },
     "execution_count": 973,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 974,
   "id": "b8d428d8-eac6-4065-ae3c-322b64210427",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "51362"
      ]
     },
     "execution_count": 974,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 916,
   "id": "f7ce015e-8479-4a0e-b499-8d07b8dcf8f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_pred = all_preds[0]\n",
    "sample_label = all_labels[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 962,
   "id": "aa6967e6-2aa7-4c23-96b4-1468474b6e33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'O',\n",
       " 1: 'B-PER',\n",
       " 2: 'I-PER',\n",
       " 3: 'B-ORG',\n",
       " 4: 'I-ORG',\n",
       " 5: 'B-LOC',\n",
       " 6: 'I-LOC',\n",
       " 7: 'B-MISC',\n",
       " 8: 'I-MISC',\n",
       " 9: '<PAD>'}"
      ]
     },
     "execution_count": 962,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reversed_ner_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 950,
   "id": "07c54c40-bbf9-40a4-9151-78199405c246",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_unpad1 = targets\n",
    "mask = label_unpad1 != 9\n",
    "label_unpad2 = label_unpad1[mask]\n",
    "# outputs = model(data, capital)\n",
    "# _, predicted = torch.max(outputs.data, 2)\n",
    "# predicted = predicted[mask]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "660eccc5-b61c-4cd2-a615-f5a11ef0c9d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_preds = itertools.chain(*all_preds)    \n",
    "all_labels =itertools.chain(*all_labels)\n",
    "result = evaluate(all_labels, all_preds,verbose=True)\n",
    "precision, recall, f1 = result[0], result[1],result[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 978,
   "id": "da98ae87-9a15-4979-9451-b3b0827b8d33",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test: \n",
      "processed 46435 tokens with 5648 phrases; found: 5666 phrases; correct: 4241.\n",
      "accuracy:  78.39%; (non-O)\n",
      "accuracy:  95.34%; precision:  74.85%; recall:  75.09%; FB1:  74.97\n",
      "              LOC: precision:  79.18%; recall:  82.55%; FB1:  80.83  1739\n",
      "             MISC: precision:  69.92%; recall:  62.25%; FB1:  65.86  625\n",
      "              ORG: precision:  69.24%; recall:  71.28%; FB1:  70.25  1710\n",
      "              PER: precision:  78.08%; recall:  76.87%; FB1:  77.47  1592\n"
     ]
    }
   ],
   "source": [
    "#evaluation\n",
    "def eval(model, loader):\n",
    "    model.eval()\n",
    "    all_preds, all_labels = [], []\n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            inputs, capitals ,targets = batch\n",
    "            #get rid of paddings on targets\n",
    "            label_unpad = targets\n",
    "            mask = label_unpad != 9\n",
    "            label_unpad = label_unpad[mask]\n",
    "            \n",
    "            outputs = model(inputs,capitals)\n",
    "            _, preds = torch.max(outputs, -1)\n",
    "            #get rid of paddings on pred\n",
    "            preds = preds[mask]\n",
    "            \n",
    "            preds_converted = [reversed_ner_tags[elem.item()] for elem in preds]\n",
    "            targets_converted = [reversed_ner_tags[elem.item()] for elem in label_unpad]\n",
    "            all_preds.extend(preds_converted)\n",
    "            all_labels.extend(targets_converted)\n",
    "    # all_preds = list(chain.from_iterable(all_preds))\n",
    "    # all_labels = list(chain.from_iterable(all_labels))\n",
    "    # all_labels = torch.cat(all_labels)\n",
    "    # all_preds = itertools.chain(*all_preds)    \n",
    "    # all_labels =itertools.chain(*all_labels)\n",
    "    result = evaluate(all_labels, all_preds,verbose=True)\n",
    "    precision, recall, f1 = result[0], result[1],result[2]\n",
    "    return precision, recall, f1\n",
    "\n",
    "print('Test: ')\n",
    "precision, recall, f1 = eval(model, test_loader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
